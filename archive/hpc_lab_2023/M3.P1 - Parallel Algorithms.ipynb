{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 3. Parallel Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Trivially Parallelizable Algorithms\n",
    "In order to properly make use of an HPC system or other parallel computing environment, you need to be able to make your code able to run in parallel. As we saw with our monte carlo pi example, even making the same algorithm run in parallel can make huge changes in terms of performance. If a job running on an HPC system has access to hundreds of cores, but is only written to run in serial, it will not make use of the computational resources it has available to it. In theory, if your task is completely parallelizable, then if you scale it up by a factor of _n_ cores, it will speed up _n_ times. That is to say, if you run a completely parallelizable job on 20 cores, it will run up to 20 times faster than on one core. \n",
    "A graph of this is below: \n",
    "\n",
    "![Theoretical maximum speedup](https://cdn.comsol.com/wordpress/2014/03/Graph-depicting-how-the-size-of-the-job-increases-with-the-number-of-available-processes.png)\n",
    "\n",
    "This graphic represents the theoretical maximum speedup of various types of HPC jobs, where the x-axis represents the number of cores the job is run on, the y axis represents how much faster the job runs (i.e. if the y-value is 20, the job is 20 times faster than single core), and the value of _phi_ represents the fraction of the job that can be parallelized. The mathematical principle this is based on is called [The Gustafson-Barsis Law](https://en.wikipedia.org/wiki/Gustafson%27s_law)\n",
    "\n",
    "The class of algorithms for which the value of _phi_ is 1, that is to say the class of algorithms that scale perfectly, is known as the \"trivially parallelizable\" class. This means that you can expect linear performance scaling as you increase number of cores linearly. These algorithms are, rather unsurprisingly, very easy to make parallel. Because of this, people using HPC systems often try to reduce their workloads to different \"building blocks\" made up of trivially parallelizable algorithms. A Trivially parallelizable algorithm is defined by task-independence. That is to say, if you break up an algorithm into tasks, in order for that algorithm to be trivially parallelizable, each task must not depend on the output of any other task. The image below represents a trivially parallelizable algorithm:\n",
    "\n",
    "![trivially parallel](http://matthewrocklin.com/slides/images/embarrassing-process.png)\n",
    "\n",
    "Each set of input data goes into a process, and comes out changed. None of them depend on what is happening in other simultaneous processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 - Generation of Data\n",
    "As an example of a trivially parallelizable algorithm, we are going to generate lots of numbers in serial and in parallel. This will also teach you a very practical skill with python's `multiprocessing` library: how to \"unroll\" a trivially parallelizable loop into processes that can run simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "CPU times: user 126 ms, sys: 102 ms, total: 228 ms\n",
      "Wall time: 209 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generating numbers in serial\n",
    "\n",
    "with open(\"./data/datagen.out\" ,\"w\") as file:\n",
    "    # Printing numbers to file\n",
    "    for i in range(1,100001):\n",
    "        file.write(str(i)+'\\n')\n",
    "        \n",
    "with open(\"./data/datagen.out\") as file:\n",
    "    # Reading numbers from file. Don't print them all out because there're too many\n",
    "    print(len(file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 ms, sys: 36.4 ms, total: 70.7 ms\n",
      "Wall time: 52.2 ms\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Generating numbers in parallel\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "file = open(\"./data/datagenparallel.out\" ,\"w\")\n",
    "\n",
    "# We simply replace the for loop with a function...\n",
    "def process_single(i):\n",
    "    file.write(str(i)+'\\n')\n",
    "\n",
    "# Create a multiprocessing Pool\n",
    "pool = Pool(32)\n",
    "\n",
    "#... And then map our inputs to it as follows:\n",
    "tasks = [pool.apply_async(process_single, (j,)) for j in range(1,1000)]\n",
    "\n",
    "# We use the `get()` function of the last submitted task to time how long it takes to run the process\n",
    "%time tasks[-1].get()\n",
    "\n",
    "#Clean up the files\n",
    "time.sleep(3)\n",
    "file.close()\n",
    "\n",
    "with open(\"./data/datagenparallel.out\") as file2:\n",
    "    # Reading numbers from file. Don't print them all out because there're too many\n",
    "    print(len(file2.readlines()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that writing the code in parallel is not much more complex than writing it in serial. In this case, the task is carried out so quickly in serial that it is not really worth it to parallelize, but there are a multitude of real-life scenarios where a workflow, or at least part of a workflow is trivially parallelizable and it makes sense to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Monte Carlo Simulations\n",
    "Remember in the last section how I made a fuss about how, even though it seems really unlikely that real world applications would ever be even close to trivially parallelizable, it's a goal that many scientists and other HPC users aim for? Well, one of the tools these people commonly use to move that direction is the _monte carlo simulation_. We've already used monte carlo simulations to compute a numerical value of pi experimentally, but we haven't really strictly defined just what exactly a monte carlo algorithm _is_, so let's do that now. A _monte carlo algorithm_ is an algorithm that attempts to produce an answer to a question by simulating many possible outcomes. For example, our monte carlo pi simulates throwing a dart at a dartboard over and over, and then uses the underlying geometry of the situation to extract a value from those darts' randomly selected locations. A monte carlo algorithm is often trivially parallelizable, because random numbers, by definintion, can always be generated independently, that is with no input from each other. For this reason, many HPC users attempt to use the monte carlo method when attempting to solve problems with \"real\" values (i.e. numerically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 - Monte Carlo Frog Hop Simulation\n",
    "To illustrate the scalability of monte carlo algorithms, we're going to use an example problem which my professor Jeff Ely uses to test the mettle of CS 1 students. Imagine a frog who is sitting on an infinite flat plane. The frog always makes jumps of length 1 unit, in a randomly selected direction. What is the probablility that the frog will be within 1 unit of its original location after _n_ jumps?\n",
    "\n",
    "You may realize that we can solve this problem with no programming at all, just math. In this case, this is true, and we can use this fact to help us check our work. Often, especially in the fields of physics and differential equations, there are problems which cannot be solved other than numerically. We will solve this problem first in serial and then we will parallelize our solution later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the frog jumps 0 times, it will land in the original circle approximately 100000 times, representing a success rate of 1.0\n",
      "If the frog jumps 1 times, it will land in the original circle approximately 100000 times, representing a success rate of 1.0\n",
      "If the frog jumps 2 times, it will land in the original circle approximately 33623 times, representing a success rate of 0.33623\n",
      "If the frog jumps 3 times, it will land in the original circle approximately 25127 times, representing a success rate of 0.25127\n",
      "If the frog jumps 4 times, it will land in the original circle approximately 20024 times, representing a success rate of 0.20024\n",
      "If the frog jumps 5 times, it will land in the original circle approximately 16674 times, representing a success rate of 0.16674\n",
      "If the frog jumps 6 times, it will land in the original circle approximately 14381 times, representing a success rate of 0.14381\n",
      "If the frog jumps 7 times, it will land in the original circle approximately 12621 times, representing a success rate of 0.12621\n",
      "If the frog jumps 8 times, it will land in the original circle approximately 11115 times, representing a success rate of 0.11115\n",
      "If the frog jumps 9 times, it will land in the original circle approximately 9854 times, representing a success rate of 0.09854\n",
      "If the frog jumps 10 times, it will land in the original circle approximately 9081 times, representing a success rate of 0.09081\n",
      "If the frog jumps 11 times, it will land in the original circle approximately 8333 times, representing a success rate of 0.08333\n",
      "If the frog jumps 12 times, it will land in the original circle approximately 7705 times, representing a success rate of 0.07705\n",
      "If the frog jumps 13 times, it will land in the original circle approximately 7104 times, representing a success rate of 0.07104\n",
      "If the frog jumps 14 times, it will land in the original circle approximately 6701 times, representing a success rate of 0.06701\n",
      "If the frog jumps 15 times, it will land in the original circle approximately 6282 times, representing a success rate of 0.06282\n",
      "If the frog jumps 16 times, it will land in the original circle approximately 5935 times, representing a success rate of 0.05935\n",
      "If the frog jumps 17 times, it will land in the original circle approximately 5604 times, representing a success rate of 0.05604\n",
      "If the frog jumps 18 times, it will land in the original circle approximately 5140 times, representing a success rate of 0.0514\n",
      "If the frog jumps 19 times, it will land in the original circle approximately 5006 times, representing a success rate of 0.05006\n",
      "CPU times: user 16.3 s, sys: 225 ms, total: 16.6 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Serial frog solution\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def monteCarlo(numJumps):\n",
    "    numTries=100000\n",
    "    lenJump=1\n",
    "    numSuccesses=0\n",
    "\n",
    "    #randomly test 1 million times and see what happens\n",
    "    for j in range(0,numTries,1):\n",
    "        #vector representing distance of 0 from origin\n",
    "        frogPosition=[0.0,0.0]\n",
    "        for i in range(0,numJumps,1):\n",
    "            #generate a random angle\n",
    "            theta=random.uniform(0,2*math.pi)\n",
    "            #add to x and y components of frog position vectors\n",
    "            frogPosition[0]+=lenJump*math.cos(theta)\n",
    "            frogPosition[1]+=lenJump*math.sin(theta)\n",
    "        #compute magnitude of final frog position vector\n",
    "        frogMagnitude=((frogPosition[0]**2+frogPosition[1]**2)**0.5)\n",
    "\n",
    "        #check if frog landed where we wanted it to\n",
    "        if frogMagnitude<=1.0:\n",
    "            #keep track of successes\n",
    "            numSuccesses+=float(1)\n",
    "    \n",
    "    #compute success rate\n",
    "    successRate=float(numSuccesses/numTries)    \n",
    "    print(\"If the frog jumps %s times, it will land in the original circle \\\n",
    "approximately %s times, representing a success rate of %s\"%(numJumps,int(numSuccesses),successRate))\n",
    "\n",
    "    \n",
    "for i in range(20):\n",
    "    monteCarlo(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the frog jumps 1 times, it will land in the original circle approximately 100000 times, representing a success rate of 1.0\n",
      "If the frog jumps 2 times, it will land in the original circle approximately 33287 times, representing a success rate of 0.33287\n",
      "If the frog jumps 3 times, it will land in the original circle approximately 25102 times, representing a success rate of 0.25102\n",
      "If the frog jumps 4 times, it will land in the original circle approximately 19885 times, representing a success rate of 0.19885\n",
      "If the frog jumps 5 times, it will land in the original circle approximately 16375 times, representing a success rate of 0.16375\n",
      "If the frog jumps 6 times, it will land in the original circle approximately 14463 times, representing a success rate of 0.14463\n",
      "If the frog jumps 7 times, it will land in the original circle approximately 12505 times, representing a success rate of 0.12505\n",
      "If the frog jumps 8 times, it will land in the original circle approximately 11110 times, representing a success rate of 0.1111\n",
      "If the frog jumps 9 times, it will land in the original circle approximately 10028 times, representing a success rate of 0.10028\n",
      "If the frog jumps 10 times, it will land in the original circle approximately 9208 times, representing a success rate of 0.09208\n",
      "If the frog jumps 11 times, it will land in the original circle approximately 8232 times, representing a success rate of 0.08232\n",
      "If the frog jumps 12 times, it will land in the original circle approximately 7823 times, representing a success rate of 0.07823\n",
      "If the frog jumps 13 times, it will land in the original circle approximately 7072 times, representing a success rate of 0.07072\n",
      "If the frog jumps 14 times, it will land in the original circle approximately 6682 times, representing a success rate of 0.06682\n",
      "If the frog jumps 15 times, it will land in the original circle approximately 6237 times, representing a success rate of 0.06237\n",
      "If the frog jumps 16 times, it will land in the original circle approximately 5968 times, representing a success rate of 0.05968\n",
      "If the frog jumps 17 times, it will land in the original circle approximately 5467 times, representing a success rate of 0.05467\n",
      "If the frog jumps 18 times, it will land in the original circle approximately 5222 times, representing a success rate of 0.05222\n",
      "If the frog jumps 19 times, it will land in the original circle approximately 5006 times, representing a success rate of 0.05006\n",
      "CPU times: user 170 ms, sys: 331 ms, total: 500 ms\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parallel frog solution\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# We parallelize the same way here, by turning for loops into functions and mapping to them\n",
    "\n",
    "def outer_loop(numJumps):\n",
    "    numTries=100000\n",
    "    lenJump=1\n",
    "    numSuccesses=0\n",
    "    #randomly test 1 million times and see what happens\n",
    "    for j in range(numTries):\n",
    "        #vector representing distance of 0 from origin\n",
    "        frogPosition=[0.0,0.0]\n",
    "        for i in range(0,numJumps,1):\n",
    "            #generate a random angle\n",
    "            theta=random.uniform(0,2*math.pi)\n",
    "            #add to x and y components of frog position vectors\n",
    "            frogPosition[0]+=lenJump*math.cos(theta)\n",
    "            frogPosition[1]+=lenJump*math.sin(theta)\n",
    "        #compute magnitude of final frog position vector\n",
    "        frogMagnitude=((frogPosition[0]**2+frogPosition[1]**2)**0.5)\n",
    "\n",
    "        #check if frog landed where we wanted it to\n",
    "        if frogMagnitude<=1.0:\n",
    "            #keep track of successes\n",
    "            numSuccesses+=float(1)\n",
    "    \n",
    "    #compute success rate\n",
    "    successRate=float(numSuccesses/numTries)    \n",
    "    print(\"If the frog jumps %s times, it will land in the original circle \\\n",
    "approximately %s times, representing a success rate of %s\"%(numJumps,int(numSuccesses),successRate))\n",
    "\n",
    "    \n",
    "pool = Pool(32)\n",
    "tasks = [pool.apply_async(outer_loop, (j,)) for j in range(1,20)]\n",
    "tasks[-1].get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the parallel version is faster than the serial version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Single Instruction, Multiple Data\n",
    "A wide variety of different HPC workflows include some slight variation on what's called a _Single Instruction, Multiple Data_ algorithm. _Single Instruction, Multiple Data_, or SIMD algorithms are considered \"nice\" algorithms in that it is very easy to write scalable code for them. This code often behaves as well as trivially parallelizable algorithms, or with specialized hardware accelerators (like General Purpose GPUs), even better. HPC accelerators and heterogeneous computing is an extremely rich and wildly fascinating topic.\n",
    "The image below represents a SIMD process:\n",
    "\n",
    "\n",
    "![simd-architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/SIMD.svg/500px-SIMD.svg.png)\n",
    "\n",
    "In this example, the \"instruction pool\" provides each of the processing units, labeled \"PU\", the same instruction, and arbitrary data from the \"data\" pool gets fed into any of the processing units as they become available\n",
    "\n",
    "SIMD algorithms are certainly something you should look for when you're trying to speed up your code. They are easy to parallelize, and many different types of problems can be reduced to them. In addition, they can often be parallelized extremely effectively, because most modern processors have hardware specifically designed to perform SIMD instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 - Vector Addition\n",
    "A common use case of parallelism in HPC is for performing vector math. Vector arithmetic comes up in all sorts of science, engineering, and other HPC applications. Many different pieces of science and math can be reduced, with some clever approximation, to vector math. In this example, we will take the sums of large vectors in serial and in parallel. Vector addition is an easily parallelizable task, because the way vector sums are computed is by taking pairwise sums of each corresponding vector subscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000000\n",
      "CPU times: user 4.83 s, sys: 754 ms, total: 5.58 s\n",
      "Wall time: 5.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vector addition in serial\n",
    "\n",
    "vec_a = [3]*22000000\n",
    "vec_b = [4]*22000000\n",
    "vec_c = [0]*22000000\n",
    "\n",
    "for i in range(len(vec_a)):\n",
    "    vec_c[i] = vec_a[i] + vec_b[i]\n",
    "\n",
    "print(len(vec_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000000\n",
      "CPU times: user 3.71 s, sys: 1.96 s, total: 5.68 s\n",
      "Wall time: 8.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vector addition in parallel\n",
    "\n",
    "vec_a = [3]*22000000\n",
    "vec_b = [4]*22000000\n",
    "vec_c = [0]*22000000\n",
    "\n",
    "def add_single(i):\n",
    "    vec_c[i] = vec_a[i] + vec_b[i]\n",
    "\n",
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(32)\n",
    "    \n",
    "pool.map(add_single, range(len(vec_a)))\n",
    "\n",
    "print(len(vec_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Multiple Instruction, Multiple Data\n",
    "SIMD algorithms are extremely performant, scalable, and easy to work with, but sometimes, you have a data pipeline where different things need to happen depending on what the data looks like. This scenario is often referred to as _Multiple Instruction, Multiple Data_, or MIMD. MIMD algorithms are much more flexible than SIMD ones, for obvious reasons. Every modern multicore computer can perform as a MIMD computer, to varying efficiencies. As of 2018, well over 95% of the TOP500 supercomputers use MIMD architectures as the basis for their computation. Of course, processors designed for MIMD computation can do SIMD computation as well, but they also have the ability to apply different kinds of logic based on data values. This ability makes MIMD extremely flexible for many diverse types of workloads. That detour aside, The image below represents a SIMD process:\n",
    "\n",
    "\n",
    "![mimd-architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/MIMD.svg/500px-SIMD.svg.png)\n",
    "\n",
    "\n",
    "In this image, multiple different types of data can come out of the \"data pool\" and are provided to the \"processing units\" along with various types of instructions from the \"instruction pool.\" Those instructions are not necessarily the same as each other, and can depend on the value of the data, randomization, or any other arbitrary logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4 - Asynchronous Branching Logic\n",
    "In this example, we are going to first, generate random numbers in parallel, and then, put them into separate files based on whether they are even or odd, and then finally, sort the files and print them out. We are going to use `multiprocessing` to perform each step in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '4', '4', '6', '8', '14', '14', '16', '16', '18', '18', '22', '24', '26', '28', '28', '30', '32', '36', '36', '40', '40', '40', '44', '46', '46', '52', '58', '64', '68', '76', '78', '78', '78', '80', '82', '84', '84', '84', '84', '84', '88', '92', '92', '94', '94', '96', '96', '98', '98']\n",
      "\n",
      "\n",
      "['1', '3', '3', '5', '5', '9', '11', '15', '17', '17', '17', '21', '23', '23', '25', '29', '31', '33', '35', '37', '39', '39', '39', '47', '51', '57', '61', '63', '65', '67', '69', '69', '71', '73', '73', '75', '77', '77', '81', '87', '87', '89', '89', '91', '95', '95', '95', '97', '99', '99']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "path = \"./data\" \n",
    "\n",
    "if os.path.isfile(path+\"/evenodd.out\"):\n",
    "    os.remove(path+\"/evenodd.out\")\n",
    "    \n",
    "if os.path.isfile(path+\"/even.out\"):\n",
    "    os.remove(path+\"/even.out\")\n",
    "\n",
    "if os.path.isfile(path+\"/odd.out\"):\n",
    "    os.remove(path+\"/odd.out\")\n",
    "\n",
    "# Write random numbers to file\n",
    "def gen_num(i):\n",
    "    file = open(path+\"/evenodd.out\", \"a\")\n",
    "    file.write(str(random.randint(0,100))+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "pool = multiprocessing.Pool(32)\n",
    "pool.map(gen_num, range(100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Put evens and odds in separate files\n",
    "def bin_out(item):\n",
    "    \n",
    "    if (int(item) % 2) == 0:\n",
    "        file2 = open(path+\"/even.out\", \"a\")\n",
    "        file2.write(str(item)+'\\n')\n",
    "        file2.close()\n",
    "    else:\n",
    "        file3 = open(path+\"/odd.out\", \"a\")\n",
    "        file3.write(str(item)+'\\n')\n",
    "        file3.close()\n",
    "\n",
    "# Reopen the file in read mode\n",
    "data = open(path+\"/evenodd.out\", \"r\").readlines()\n",
    "pool = multiprocessing.Pool(32)\n",
    "pool.map(bin_out, data)\n",
    "\n",
    "# sort the files\n",
    "def keyfunc(line):\n",
    "    try:\n",
    "        return int(line)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "#Sort the files in place\n",
    "for file in [path+\"/even.out\", path+\"/odd.out\"]:\n",
    "    with open(file) as fin:\n",
    "        content = sorted(fin, key=keyfunc)\n",
    "\n",
    "    with open(file, \"w\") as fout:\n",
    "        fout.writelines(content)\n",
    "\n",
    "# Show that it's ordered\n",
    "for file in [path+\"/even.out\", path+\"/odd.out\"]:\n",
    "    with open(file) as fin:\n",
    "        print([ x.strip() for x in fin.readlines() if x != \"\\n\" ])\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Concurrency vs. Parallelism\n",
    "You may have heard the terms _concurrency_ and _parallelism_ before. They are often confused, and this makes sense because they both relate to running more than one process at a time. Concurrency is the composition of independently executing processes, while parallelism is the simultaneous execution of (possibly related) computations. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. Another reason they often get conflated is that they are often used together. \n",
    "\n",
    "A purely SIMD algorithm is a parallel environment, while any kind of MIMD algorithm includes at least some concurrency. Another time when concurrency is useful is when the workload you are trying to run involves interprocess communication, waiting for hard disk access, or waiting for network access. The reason for this is that it takes a lot of time to access the network, and a clever programmer will allow another process to use the spare CPU cycles that would be otherwise spent waiting. Hopefully, you can see how there is potential for overlap of parallelism and concurrency. \n",
    "\n",
    "Imagine a scenario where you want to generate files by connecting to an external service (maybe you're asking the Google maps API where something is), and then you want to perform some computation on those results. A purely parallel approach would be to make all of the Google API requests (all in parallel), wait for them all to finish, and then do all of the computation on each response, again in parallel. A better approach, one that uses concurrency, would be to make all of the API requests, and while some of the slower requests are waiting for their responses, allow the faster requests to begin their computation.\n",
    "\n",
    "The following image describes parallelism and concurrency:\n",
    "\n",
    "![parallelism and concurrency](https://pbs.twimg.com/media/DSFCqf2U8AAjgqI.jpg)\n",
    "\n",
    "Using this image as a reference, we can imagine a scenario where there are multiple CPUs, as in the bottom image, and there are multiple queues per CPU, as in the top image. This would be a combination of parallelism and concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5a - Network Interaction and IO-Bound Tasks\n",
    "As mentioned in the previous section, many of the tasks that lend themselves to concurrent execution are tasks with network or filesystem IO. This is because the amount of time it takes to connect to the external internet, or even the time it takes to connect to the local filesystem is much longer than the CPU's instruction cycle. This causes long periods of time (up to many seconds or minutes, depending on the task), where the CPU is just waiting for things to happen. Because of this, you are able to run many of these tasks all at the same time, because some can process while others wait, and vice-versa. In situations like this, you can run many more of these tasks than you have cores on your computer and expect reasonable performance speedups.\n",
    "\n",
    "In this example, we will create one sleep job and demonstrate that we can run sleep jobs concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    " \n",
    "NUM_WORKERS = 32\n",
    " \n",
    "def only_sleep():\n",
    "    \"\"\" Do nothing, wait for a timer to expire \"\"\"\n",
    "    print(\"PID: %s, Process Name: %s, Thread Name: %s\" % (\n",
    "        os.getpid(),\n",
    "        multiprocessing.current_process().name,\n",
    "        threading.current_thread().name)\n",
    "    )\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-28\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-29\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-30\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-31\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-32\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-33\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-34\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-35\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-36\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-37\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-38\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-39\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-40\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-41\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-42\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-43\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-44\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-45\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-46\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-47\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-48\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-49\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-50\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-51\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-52\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-53\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-54\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-55\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-56\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-57\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-58\n",
      "PID: 908, Process Name: MainProcess, Thread Name: Thread-59\n",
      "Threads time= 1.1141877174377441 seconds\n"
     ]
    }
   ],
   "source": [
    "## Run tasks serially\n",
    "\n",
    "serial_run = False # Change to True if you want the serial version to run\n",
    "                   # It will take about 30 seconds\n",
    "\n",
    "if serial_run:\n",
    "    start_time = time.time()\n",
    "    for _ in range(NUM_WORKERS):\n",
    "        only_sleep()\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Serial time= {}\".format(end_time - start_time))\n",
    " \n",
    "# Run tasks using threads\n",
    "# Note that only one thread can run at a time, so this really is concurrency \n",
    "start_time = time.time()\n",
    "threads = [threading.Thread(target=only_sleep) for _ in range(NUM_WORKERS)]\n",
    "[thread.start() for thread in threads]\n",
    "[thread.join() for thread in threads]\n",
    "end_time = time.time()\n",
    " \n",
    "print(\"Threads time= {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5b - Number Crunching and CPU-Bound Tasks\n",
    "Algorithms which can be run concurrently to great effect are often nice to work with. This is because they are, by definition, easy to speed up without too many resources. I like to refer to another class of easy to speed up tasks as _number crunching_. Tasks that fit into this category must a) have little or no input data, b) produce easy to handle amounts of output data, and c) use up at least an entire compute core for most of the time it needs to be running. Because of these requirements, Number crunching tasks are easy to speed up through parallelization, by adding more cores to a pool. They are slightly harder to speed up than tasks from the previous example, because they require additional hardware for more speedup, but they are still easy to speed up overall.\n",
    "\n",
    "In this example, we will create a number crunching task and show that it benefits from being run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number crunching workload\n",
    "\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "def crunch_numbers():\n",
    "    \"\"\" Do some computations \"\"\"\n",
    "    print(\"PID: %s, Process Name: %s, Thread Name: %s\" % (\n",
    "        os.getpid(),\n",
    "        multiprocessing.current_process().name,\n",
    "        threading.current_thread().name)\n",
    "    )\n",
    "    x = 0\n",
    "    while x < 10000000:\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_run = False # Change to True if you want the threads version to run\n",
    "                   # It will take about 30 seconds\n",
    "\n",
    "# This is here to illustrate that this task is NOT concurrent. \n",
    "# It will scale ONLY with increased parallelism\n",
    "if thread_run:\n",
    "    start_time = time.time()\n",
    "    threads = [threading.Thread(target=crunch_numbers) for _ in range(NUM_WORKERS)]\n",
    "    [thread.start() for thread in threads]\n",
    "    [thread.join() for thread in threads]\n",
    "    end_time = time.time()\n",
    "    print(\"Threads time= {} seconds\".format(end_time - start_time))\n",
    "\n",
    "# Using almost the same workload from above, we will parallelize the crunch_numbers function\n",
    "\n",
    "start_time = time.time()\n",
    "processes = [multiprocessing.Process(target=crunch_numbers) for _ in range(NUM_WORKERS)]\n",
    "[process.start() for process in processes]\n",
    "[process.join() for process in processes]\n",
    "end_time = time.time()\n",
    " \n",
    "print(\"Parallel time = {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Dataflows\n",
    "\n",
    "A _dataflow_ is a set of different algorithms, which may or may not be parallel, that all fit together in some way, with the output of some parts leading to the input of other parts. Most real world HPC applications are not just simple SIMD or MIMD blocks, but are instead a number of different pieces of code, all of which produce data based on other applications or input data. Each of those individual pieces of code may be SIMD or concurrent, or trivially parallel, or monte carlo, or any other kind of code, but the important part is understanding how they all work together. The dataflow is an incredibly powerful way of \"gluing\" many programs together into one program that carries out exactly what you want it to. The image below is a representation of a dataflow:\n",
    "\n",
    "![basic dataflow](http://www.digitaleng.news/de/wp-content/uploads/2016/10/HPC-Workflow.jpg)\n",
    "\n",
    "\n",
    "In this case, data is first generated, then the quality of that data is assured, and then the data gets computed on, creating some output, and finally, that data gets visualized and consolidated to a human-readable format. This is a fairly common type of dataflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6 - `sleep_fail` dataflow\n",
    "The dataflow presented in the previous section is a relatively typical dataflow, but it's also farily complex. To give you a more gentle introduction to working with dataflows, we are going to design a dataflow called `sleep_fail`. This dataflow consists of a data generation layer, in which we define a function which sleeps for _n_ (user input) seconds before failing with a probability _p_ (also user input). Then, it writes whether it failed or succeeded to a file. Then, we have a data consolidation layer, which reads that file and prints out how many times it ran, succeeded, and failed. The resulting workflow looks a bit like this:\n",
    "```\n",
    "    sleep1  sleep2 ...sleepN\n",
    "      |       |        |\n",
    "      V       V        v\n",
    "       \\      |       /\n",
    "        \\     |      /\n",
    "          sleep_Final\n",
    "```\n",
    "Note that this dataflow is serial. This is intended so that we can introduce you to the concept of a dataflow without worrying about parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define sleep_fail function\n",
    "def sleep_fail(n, p, path):\n",
    "    time.sleep(n)\n",
    "    if random.random() < p:\n",
    "        file = open(path, \"a\")\n",
    "        file.write(\"Exception\\n\")\n",
    "        file.close()\n",
    "        raise Exception\n",
    "    else:\n",
    "        file = open(path, \"a\")\n",
    "        file.write(\"Success\\n\")\n",
    "        file.close()\n",
    "        \n",
    "# Define summarize_sleeps\n",
    "def summarize_sleeps(path):\n",
    "    file = open(path, \"r\")\n",
    "    succeed = 0\n",
    "    exceptio = 0\n",
    "    for line in file.readlines():\n",
    "        if \"Exception\" in line:\n",
    "            exceptio +=1\n",
    "        elif \"Success\" in line:\n",
    "            succeed +=1\n",
    "    print(\"Failed {} Times\".format(exceptio))\n",
    "    print(\"Passed {} Times\".format(succeed))\n",
    "    print(\"Ran {} Times\".format(succeed + exceptio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"./data/sleep_fail.out\"\n",
    "\n",
    "if os.path.isfile(path):\n",
    "    os.remove(path)\n",
    "\n",
    "for i in range(500):\n",
    "    try:\n",
    "        sleep_fail(0, 0.2, path)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "summarize_sleeps(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise: Write a dataflow that estimates the value of _e_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
