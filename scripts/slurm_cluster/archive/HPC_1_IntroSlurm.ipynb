{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a266fc-c655-4854-895a-5be813efd7ef",
   "metadata": {},
   "source": [
    "# Compiling and Running Programs on an HPC System\n",
    "\n",
    "This notebook will guide you through the steps necessary to compile and run a computationally intensive C program on a High-Performance Computing (HPC) system. We will cover both basic and advanced topics, focusing on using specific compilers and modules available on the HPC.\n",
    "\n",
    "## Why Use an HPC for Compiling?\n",
    "\n",
    "Compiling and running programs on an HPC system can significantly enhance performance for compute-intensive tasks. This is due to several advantages that HPC systems provide:\n",
    "- **Access to specialized compilers and libraries:** Optimized to exploit the hardware capabilities like multiple cores, high-performance GPUs, and fast interconnects.\n",
    "- **Module systems for easy software management:** Allows users to easily load and switch between different software environments and libraries needed for different applications.\n",
    "- **Enhanced computational power:** With more processors, memory, and storage than a typical desktop or laptop, HPC systems can handle much larger computations.\n",
    "\n",
    "## Example Program: `calculate_pi.c`\n",
    "\n",
    "Instead of a simple hello world program, we will use a more complex C program that calculates the value of Pi using the Monte Carlo method. This method involves simulating random points and assessing how many fall within a quarter circle inscribed in a unit square. The ratio of points inside the circle to the total points approximates Pi/4.\n",
    "\n",
    "Here's the source code for `calculate_pi.c`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2822b32b-9b1c-4ac1-bb25-231ca6d2320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex C program written to calculate_pi.c with command-line argument support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for the C program file\n",
    "c_program_path = \"calculate_pi.c\"\n",
    "\n",
    "# Remove the existing file if it exists\n",
    "if os.path.exists(c_program_path):\n",
    "    os.remove(c_program_path)\n",
    "\n",
    "# Create and write the C program\n",
    "c_program = \"\"\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    if (argc < 2) {\n",
    "        fprintf(stderr, \"Usage: %s <iterations>\\\\n\", argv[0]);\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    int iterations = atoi(argv[1]);\n",
    "    if (iterations <= 0) {\n",
    "        fprintf(stderr, \"Please provide a positive integer for iterations.\\\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    int inside = 0;\n",
    "    double x, y, pi;\n",
    "\n",
    "    srand(time(NULL)); // Seed the random number generator\n",
    "\n",
    "    for (int i = 0; i < iterations; i++) {\n",
    "        x = (double)rand() / RAND_MAX;\n",
    "        y = (double)rand() / RAND_MAX;\n",
    "        if (x * x + y * y <= 1) {\n",
    "            inside++;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pi = (double)inside / iterations * 4;\n",
    "    printf(\"Approximation of Pi: %f\\\\n\", pi);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Write the C program to a file\n",
    "with open(c_program_path, \"w\") as file:\n",
    "    file.write(c_program)\n",
    "\n",
    "print(f\"Complex C program written to {c_program_path} with command-line argument support.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774df685-eb5b-48e2-9e63-7baf0430cd7d",
   "metadata": {},
   "source": [
    "### 2. Compile the Program\n",
    "\n",
    "Use the `gcc` command to compile `calculate_pi.c` and generate an executable named `calculate_pi`:\n",
    "\n",
    "1. Load the Necessary Modules\n",
    "HPC systems use module systems to manage software environments. Before compiling, load the appropriate compiler module. Here, we'll use the GCC compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7756cb5a-cfcf-45a7-935e-c0bcb40a6bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the C program...\n",
      "Compilation successful, executable 'calculate_pi' created.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Compile the C program using gcc\n",
    "compile_command = \"gcc calculate_pi.c -o calculate_pi\"  # Corrected output file name\n",
    "compile_process = subprocess.run(compile_command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and error (if any) after compilation attempt\n",
    "print(\"Compiling the C program...\")\n",
    "if compile_process.stdout:\n",
    "    print(\"Output:\", compile_process.stdout)\n",
    "if compile_process.stderr:\n",
    "    print(\"Error:\", compile_process.stderr)\n",
    "\n",
    "# Check if the executable was created\n",
    "if os.path.exists(\"calculate_pi\"):  # Corrected executable file name\n",
    "    print(\"Compilation successful, executable 'calculate_pi' created.\")\n",
    "else:\n",
    "    print(\"Compilation failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035dd19-fec1-4f75-95f5-1b7fa14b3aaf",
   "metadata": {},
   "source": [
    "### Run the Program\n",
    "\n",
    "Now we will execute the program. \n",
    "\n",
    "**As it is doing  100000000 ITERATIONS it will take time, Be patient!** \n",
    "\n",
    "Execute the program with the following command to see the output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b0efad-3f4e-4560-b429-6f2e41c8affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of Pi: 3.141493\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Compile the C program first if it hasn't been compiled\n",
    "compile_command = [\"gcc\", \"calculate_pi.c\", \"-o\", \"calculate_pi\"]\n",
    "subprocess.run(compile_command)\n",
    "\n",
    "# Run the compiled program\n",
    "run_program = subprocess.run([\"./calculate_pi\", \"100000000\"], capture_output=True, text=True)\n",
    "\n",
    "# Print the output of the program\n",
    "print(run_program.stdout)\n",
    "print(run_program.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49313d-0058-4546-9ea8-68c0bfe973ea",
   "metadata": {},
   "source": [
    "# Resource managers and Slurm\n",
    "## What is a Resource Manager?\n",
    "An HPC system is made up of smaller constituent systems all working together. Normally, all of our interactions  are with one computer, which is the login node of a cluster. This is because we have not yet learned to use a _resource manager_. A _resource manager_ is a program that contains both a server, running on a head node, and any number of clients, running on worker nodes. The client allows worker nodes to ask the head node for work, and the server provides jobs to carry out. Almost all clusters have some form of resource manager on them which allows users to submit and monitor jobs to be run on the worker nodes. Most resource managers also have scheduling systems which allow them to run jobs in different orders based on a number of parameters. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97208cc-5c67-495b-9bde-b275a2dc1dce",
   "metadata": {},
   "source": [
    "## What is an HPC Cluster?\n",
    "\n",
    "A High-Performance Computing (HPC) cluster is a collection of interconnected computers that work together to perform complex computations. Each computer in the cluster is known as a node, and the nodes are connected through a high-speed network.\n",
    "\n",
    "### Key Components of an HPC Cluster\n",
    "- **Login Nodes**: Used for preparing jobs and submitting them to the scheduler. Not for running heavy computations.\n",
    "- **Compute Nodes**: Dedicated to running computational jobs.\n",
    "- **Scheduler**: Manages job submission, resource allocation, and job execution. SLURM is a popular scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac6078-87af-41da-901c-a9605d5fd4f2",
   "metadata": {},
   "source": [
    "## Introduction to SLURM\n",
    "\n",
    "SLURM (Simple Linux Utility for Resource Management) is a powerful scheduler that helps manage resources and schedule jobs on an HPC cluster.\n",
    "\n",
    "The following image describes the job flow of Slurm, a commonly used resource manager:\n",
    "\n",
    "![SLURM architecture](https://slurm.schedmd.com/arch.gif)\n",
    "\n",
    "### Main SLURM Commands\n",
    "- `srun`: Run parallel jobs.\n",
    "- `sbatch`: Submit a batch job script to the scheduler.\n",
    "- `squeue`: View the job queue.\n",
    "- `scancel`: Cancel a job.\n",
    "- `sinfo`: View information about the nodes and partitions.\n",
    "\n",
    "In this notebook, we will create, compile, and run a simple C program using SLURM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb088d4-26d6-4f5a-8245-dec483ff18c7",
   "metadata": {},
   "source": [
    "# Understanding Cluster Configuration with `sinfo`\n",
    "\n",
    "The `sinfo` command in SLURM provides detailed information about the current state of the nodes and partitions within the HPC cluster. This command is essential for users to understand the availability and status of resources before submitting jobs.\n",
    "\n",
    "## Key Outputs of `sinfo`\n",
    "\n",
    "- **PARTITION**: Shows the partition names.\n",
    "- **AVAIL**: Indicates if the partition is available (`up`) or not (`down`).\n",
    "- **TIMELIMIT**: Lists the maximum time that jobs are allowed to run in the partition.\n",
    "- **NODES**: Shows the number of nodes in each state.\n",
    "- **STATE**: Indicates the state of the nodes (e.g., `idle`, `alloc` for allocated, etc.).\n",
    "- **NODELIST**: Provides the specific names or identifiers of the nodes.\n",
    "\n",
    "By default, `sinfo` displays a brief summary. To get more detailed information, you can use various flags with this command.\n",
    "\n",
    "## Example Commands\n",
    "\n",
    "- `sinfo`: Provides a basic overview of the cluster.\n",
    "- `sinfo -l`: Provides a detailed view.\n",
    "- `sinfo -N`: Lists information node by node.\n",
    "- `sinfo -s`: Displays a short format.\n",
    "\n",
    "Let's run a basic `sinfo` command to see the current state of the cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e559c3-1386-44ce-9179-e8593ba79cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a43955-4f63-4c77-98a5-c04dfb6c07ba",
   "metadata": {},
   "source": [
    "## Creating and Submitting a SLURM Job\n",
    "\n",
    "Users submit tasks to a queue, which are then ordered by priority rules set by administrators, and those jobs get run on any available backend resources.\n",
    "\n",
    "\n",
    "**srun** is used to submit a job for execution in real time\n",
    "\n",
    "while\n",
    "\n",
    "**sbatch** is used to submit a job script for later execution.\n",
    "\n",
    "They both accept practically the same set of parameters. The main difference is that srun is interactive and blocking (you get the result in your terminal and you cannot write other commands until it is finished), while sbatch is batch processing and non-blocking (results are written to a file and you can submit other commands right away).\n",
    "\n",
    "If you use **srun** in the background with the & sign, then you remove the 'blocking' feature of srun, which becomes interactive but non-blocking. It is still interactive though, meaning that the output will clutter your terminal, and the srun processes are linked to your terminal. If you disconnect, you will loose control over them, or they might be killed (depending on whether they use stdout or not basically). And they will be killed if the machine to which you connect to submit jobs is rebooted.\n",
    "To run our compiled program on the HPC cluster, we need to create a SLURM job script. This script specifies the resources required and the commands to execute.\n",
    "\n",
    "### SLURM Job Script Example\n",
    "Below is a simple SLURM script that requests 1 compute node for 5 minutes and runs our `hello_hpc` executable.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=calculate_pi\n",
    "#SBATCH --output=calculate_pi.out\n",
    "#SBATCH --error=calculate_pi.err\n",
    "#SBATCH --time=00:05:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --mem=1G  # Allocates 1 GB of total memory to the job\n",
    "\n",
    "# Load necessary modules\n",
    "module load gcc\n",
    "\n",
    "# Run the executable\n",
    "srun ./calculate_pi 1000000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c34e823c-82e3-4165-92b1-b602e26cbdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM job script written to calculate_pi.slurm.\n",
      "\n",
      "Contents of the SLURM job script:\n",
      "----------------------------------\n",
      "#!/bin/bash\n",
      "#SBATCH --job-name=calculate_pi\n",
      "#SBATCH --output=calculate_pi.out\n",
      "#SBATCH --error=calculate_pi.err\n",
      "#SBATCH --time=00:05:00\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --mem=1G  # Allocates 1 GB of total memory to the job\n",
      "\n",
      "# Load necessary modules\n",
      "module load gcc\n",
      "\n",
      "# Run the executable\n",
      "srun ./calculate_pi 1000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the SLURM job script path\n",
    "slurm_script_path = \"calculate_pi.slurm\"\n",
    "\n",
    "# Remove existing SLURM script if it exists\n",
    "if os.path.exists(slurm_script_path):\n",
    "    os.remove(slurm_script_path)\n",
    "\n",
    "# Create the SLURM job script with explicit path to bash\n",
    "slurm_script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=calculate_pi\n",
    "#SBATCH --output=calculate_pi.out\n",
    "#SBATCH --error=calculate_pi.err\n",
    "#SBATCH --time=00:05:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --mem=1G  # Allocates 1 GB of total memory to the job\n",
    "\n",
    "# Load necessary modules\n",
    "module load gcc\n",
    "\n",
    "# Run the executable\n",
    "srun ./calculate_pi 1000000000\n",
    "\"\"\"\n",
    "\n",
    "# Write the SLURM job script to a file\n",
    "with open(slurm_script_path, \"w\") as file:\n",
    "    file.write(slurm_script)\n",
    "\n",
    "# Confirm the file has been written\n",
    "print(f\"SLURM job script written to {slurm_script_path}.\")\n",
    "\n",
    "# Make the script executable\n",
    "os.chmod(slurm_script_path, 0o755)\n",
    "\n",
    "# Read and print the contents of the SLURM job script\n",
    "with open(slurm_script_path, \"r\") as file:\n",
    "    script_content = file.read()\n",
    "\n",
    "print(\"\\nContents of the SLURM job script:\")\n",
    "print(\"----------------------------------\")\n",
    "print(script_content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff1901-161c-4d81-9332-0b8d516703dd",
   "metadata": {},
   "source": [
    "### Submitting and Monitoring a SLURM Job in Jupyter Notebook\n",
    "\n",
    "This section of the notebook demonstrates how to submit a SLURM job using the `sbatch` command and monitor its status using the `squeue` command. We will execute these commands directly from the Jupyter Notebook using the `!` syntax, which allows us to run shell commands in a more interactive manner.\n",
    "\n",
    "#### Submitting the SLURM Job\n",
    "\n",
    "We use the `sbatch` command to submit a job to the SLURM scheduler. The job script `calculate_pi.slurm` contains instructions for the SLURM workload manager on how to execute the task. This script specifies the resources needed and the executable to run.\n",
    "\n",
    "#### Allowing Time for Job Queueing\n",
    "To ensure that the job is queued before we check its status, we include a short delay using Python's time.sleep() function. This is crucial as SLURM may take a few moments to update the queue, especially in busy environments.\n",
    "\n",
    "#### Checking the Job Status\n",
    "After submitting the job, we use the squeue command to check the status of jobs in the queue. This command lists all jobs that are currently queued or running, allowing us to monitor the status of our job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e49c708a-da6f-4c00-854f-4b4f080c411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 47\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                31 cpubase_b spawner-  user001  R      49:39      1 node1\n",
      "                47 cpubase_b calculat  user001  R       0:03      1 node2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Submit the SLURM job using the `!` syntax for direct shell command execution\n",
    "!sbatch {\"calculate_pi.slurm\"}\n",
    "\n",
    "# Wait for a few seconds to ensure the job is queued\n",
    "time.sleep(3)\n",
    "\n",
    "# Check the status of the job queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acaf84a-89a7-4b49-9c10-d39aa9696314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "454e2deb-7cb3-412e-887d-2036896c0c2a",
   "metadata": {},
   "source": [
    "### Examining SLURM Job Output and Error Files\n",
    "\n",
    "Once a SLURM job is submitted and executed, it generates output and error files specified in the job script. These files contain valuable information about the execution of the program, including any results printed to the console and any error messages that occurred during execution.\n",
    "\n",
    "#### Understanding Output and Error Files\n",
    "\n",
    "##### Output File (`calculate_pi.out`) **\n",
    "\n",
    "The output file contains the standard output from the program execution. This includes any `printf` statements or other console outputs generated by the C program. In our case, this file will contain the approximate value of Pi calculated by our program.\n",
    "\n",
    "##### Error File (`calculate_pi.err`)\n",
    "\n",
    "The error file captures any standard error messages produced by the program. This includes any compilation or runtime errors, warnings, or other messages that are sent to the error stream.\n",
    "\n",
    "#### Code to Display the Contents of Output and Error Files\n",
    "\n",
    "Let's write code to read and display the contents of these files, allowing us to verify the results and diagnose any potential issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5cf4202-bcaf-4e31-a1bf-fe05dc9fe8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of calculate_pi.out:\n",
      "----------------------------------\n",
      "Approximation of Pi: 3.141561\n",
      "\n",
      "\n",
      "Contents of calculate_pi.err:\n",
      "----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths to the output and error files\n",
    "output_file = \"calculate_pi.out\"\n",
    "error_file = \"calculate_pi.err\"\n",
    "\n",
    "# Check and display the contents of the output file\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"\\nContents of {output_file}:\")\n",
    "    print(\"----------------------------------\")\n",
    "    with open(output_file, \"r\") as file:\n",
    "        output_content = file.read()\n",
    "        print(output_content)\n",
    "else:\n",
    "    print(f\"\\n{output_file} does not exist.\")\n",
    "\n",
    "# Check and display the contents of the error file\n",
    "if os.path.exists(error_file):\n",
    "    print(f\"\\nContents of {error_file}:\")\n",
    "    print(\"----------------------------------\")\n",
    "    with open(error_file, \"r\") as file:\n",
    "        error_content = file.read()\n",
    "        print(error_content)\n",
    "else:\n",
    "    print(f\"\\n{error_file} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43cf51-c75a-4b1e-aa0d-4dfb5ad771ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecfb796b-49a0-48ca-bb23-41a6cde435b4",
   "metadata": {},
   "source": [
    "# Understanding `srun` in SLURM\n",
    "\n",
    "In SLURM, both `sbatch` and `srun` are used to execute jobs on an HPC cluster, but they serve different purposes and are used in distinct scenarios. Understanding when to use each command is essential for effective job management and resource utilization.\n",
    "\n",
    "## `sbatch` vs. `srun`\n",
    "\n",
    "### `sbatch`\n",
    "\n",
    "- **Purpose**: Submits a batch job script to the scheduler to be executed at a later time when resources become available.\n",
    "- **Usage**: Primarily used for batch processing of non-interactive tasks, where you write a script with job specifications and submit it to the queue.\n",
    "- **Execution**: The job runs according to the specified resources and constraints in the SLURM script without user interaction during execution.\n",
    "\n",
    "### `srun`\n",
    "\n",
    "- **Purpose**: Launches parallel tasks and can be used for both interactive and non-interactive job execution.\n",
    "- **Usage**: Often used for interactive jobs or to launch parallel tasks within an already scheduled job.\n",
    "- **Execution**: `srun` can be used to run tasks interactively on compute nodes or to start tasks within a running job environment, providing more flexibility for dynamic task execution.\n",
    "\n",
    "## When to Use `srun`\n",
    "\n",
    "- **Interactive Jobs**: Use `srun` to start an interactive session on a compute node for testing, debugging, or running tasks interactively.\n",
    "- **Within Scripts**: Use `srun` within an `sbatch` script to launch parallel tasks that require coordination across multiple CPUs or nodes.\n",
    "- **Dynamic Execution**: Use `srun` to dynamically allocate resources and run tasks without needing to pre-write a batch script.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "We will demonstrate how to use `srun` to run a simple interactive job and a parallel computation task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f32c2-d01b-46cc-97f1-31da2affd4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]0;user001@node1:~/hpc_lab_24\u0007[user001@node1 hpc_lab_24]$ "
     ]
    }
   ],
   "source": [
    "# Use srun to start an interactive session on a compute node\n",
    "# Note: This command is typically run in a terminal, not directly executable in a Jupyter Notebook.\n",
    "\n",
    "!srun --pty bash -i\n",
    "\n",
    "# Explanation:\n",
    "# --pty: Allocates a pseudo-terminal from the compute node allocated, allowing interactive command execution.\n",
    "# bash -i: Starts an interactive bash shell session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434cdf78-f56b-4088-ac11-2a70dabaf909",
   "metadata": {},
   "source": [
    "## Interactive SLURM Usage in Jupyter Terminal\n",
    "\n",
    "This guide will help you explore SLURM commands interactively within a Jupyter terminal. By practicing these commands, you'll gain familiarity with job scheduling, monitoring, and resource management on an HPC cluster.\n",
    "\n",
    "### 1. Access the Shell in Jupyter\n",
    "\n",
    "#### Open a New Terminal\n",
    "\n",
    "- **Open a Launcher**: Click on the `+` icon or `New Launcher` to open the launcher.\n",
    "- **Select Terminal**: From the launcher, click on `Terminal` to open a new shell session. This terminal acts like a login node interface.\n",
    "\n",
    "### 2. Run Basic Linux Commands\n",
    "\n",
    "Before diving into SLURM, familiarize yourself with some basic Linux commands to navigate and manage your files.\n",
    "\n",
    "- **List Files and Directories**: \n",
    "    - Run the command `ls` to show the content of the current folder.\n",
    "  \n",
    "- **Print Current Directory**:\n",
    "    - Run the command `pwd` to display the current directory path.\n",
    "\n",
    "### 3. SLURM Commands for Job Management\n",
    "\n",
    "Learn how to interact with SLURM to manage and monitor your computational jobs.\n",
    "\n",
    "- **Check Available Partitions**:\n",
    "  - Run the command `sinfo` to display available partitions and their status. This is useful for determining resource availability and node types.\n",
    "\n",
    "- **View Job Queue**:\n",
    "  - Run the command `squeue` to show the current job queue. This command displays jobs currently running or waiting, along with their IDs, user names, and statuses.\n",
    "\n",
    "- **Submit a Job Script**:\n",
    "  - Use the command `sbatch calculate_pi.slurm` to submit a batch job to the SLURM scheduler for execution when resources are available. Replace `calculate_pi.slurm` with the name of your actual job script.\n",
    "\n",
    "- **Check Your Job Status**:\n",
    "  - Use `squeue -u $USER` to list all jobs submitted by the current user, allowing you to monitor their progress and status.\n",
    "\n",
    "- **Cancel a Job**:\n",
    "  - Run `scancel <job_id>` to cancel a job specified by its job ID. Replace `<job_id>` with the actual job ID you wish to cancel.\n",
    "\n",
    "### 4. Running Interactive Jobs\n",
    "\n",
    "Explore interactive job sessions to dynamically test and run tasks on compute nodes.\n",
    "\n",
    "- **Start an Interactive Session**:\n",
    "  - Use `srun --pty bash -i` to allocate resources and start an interactive bash session on a compute node. This is ideal for debugging and interactive computations.\n",
    "\n",
    "  **What You Can Do**:\n",
    "  - Run commands interactively.\n",
    "  - Test scripts with immediate feedback.\n",
    "  - Explore resource usage in real-time.\n",
    "\n",
    "### 5. Analyze Job Performance with `sacct`\n",
    "\n",
    "After jobs have completed, use `sacct` to gather detailed information about their execution.\n",
    "\n",
    "- **View Completed Job Details**:\n",
    "  - Run `sacct --format=JobID,JobName,User,State,Elapsed,CPUTime,MaxRSS` to provide detailed statistics for completed jobs, such as CPU time, memory usage, and job state. This helps in understanding job performance and resource utilization.\n",
    "\n",
    "## Discussion and Reflection\n",
    "\n",
    "- **Efficiency**: Reflect on how interactive SLURM commands enhance your ability to manage computational workloads effectively.\n",
    "- **Troubleshooting**: Consider how interactive sessions can assist in diagnosing job issues and refining scripts.\n",
    "- **Further Exploration**: Explore additional SLURM commands and options to optimize job scheduling and resource allocation.\n",
    "\n",
    "By following this guide, you will gain hands-on experience with SLURM and Linux shell commands, equipping you with the skills needed to navigate and utilize HPC resources effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d0c6c-965b-4904-8196-3e1a2cd98432",
   "metadata": {},
   "source": [
    "## Understanding `sacct` in SLURM\n",
    "\n",
    "The `sacct` command in SLURM is used to report accounting information about jobs and job steps that are managed by the SLURM workload manager. It provides detailed information about the jobs, such as resource usage, runtime statistics, and job states, which are crucial for performance analysis and optimization.\n",
    "\n",
    "### Key Features of `sacct`\n",
    "\n",
    "- **Job and Step Information**: `sacct` provides data on both jobs and individual job steps, offering insights into how resources were utilized at each stage of execution.\n",
    "- **Comprehensive Metrics**: Reports on CPU time, memory usage, job states, exit codes, and more, helping users identify bottlenecks or inefficiencies.\n",
    "- **Historical Data**: Accesses records of past jobs, allowing users to review previous job performances and resource consumption.\n",
    "\n",
    "### Common `sacct` Options\n",
    "\n",
    "- `-j <job_id>`: Specifies a particular job ID to retrieve information for that job.\n",
    "- `--format`: Customizes the output format by specifying the fields to display.\n",
    "- `--starttime`: Limits the report to jobs that started after a specified time.\n",
    "- `-a` or `--allusers`: Displays information for all users (requires admin privileges).\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "We'll demonstrate how to use `sacct` to view detailed information about completed jobs, including custom formatting options.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa6e41-6a16-4020-a2d4-4925d3d8131f",
   "metadata": {},
   "source": [
    "#### Retrieving Job IDs of Previous Jobs Using `sacct`\n",
    "\n",
    "The `sacct` command is useful for retrieving detailed information about past jobs, including their job IDs, which can be essential for tracking, debugging, or resubmitting jobs. By querying jobs based on a specific start time or other criteria, we can easily identify and work with past job records.\n",
    "\n",
    "##### Example: Retrieve Job IDs for Jobs Started After a Specific Date\n",
    "\n",
    "In this example, we will use `sacct` to list jobs that started after a specified date, focusing on displaying their job IDs, names, and other relevant metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce771298-0cc4-48aa-aa35-b4b2ef472eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sacct to view jobs started after a specific date, focusing on retrieving job IDs\n",
    "!sacct --starttime=2024-08-01 --format=JobID,JobName,User,State,Elapsed,CPUTime,MaxRSS\n",
    "\n",
    "# Explanation:\n",
    "# --starttime=2024-08-01: Restricts the report to jobs that started on or after August 1, 2024.\n",
    "#  --format: Customizes the output to show JobID, JobName, User, State, Elapsed time, CPUTime, and MaxRSS (maximum resident set size).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61473bb-4b4b-4135-ac12-815ad3d7e13e",
   "metadata": {},
   "source": [
    "### Here we use the job id 12, update it with any job id you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9d5a5-b66a-4b16-921b-f09c348232b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sacct to retrieve information about the most recent jobs\n",
    "# Note: This command is typically run in a terminal or a Jupyter Notebook with shell access.\n",
    "\n",
    "!sacct -j 42 --format=JobID,JobName,User,State,Time,MaxRSS,MaxVMSize\n",
    "\n",
    "# Explanation:\n",
    "# -j <job_id>: Replace <job_id> with your job ID to view specific job details.\n",
    "# --format: Specifies the fields to display. In this example, JobID, JobName, User, State, Time, MaxRSS (maximum resident set size), and MaxVMSize (maximum virtual memory size) are shown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61153e-ef41-4955-9c2e-b54cddf0c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sacct to view jobs started after a specific date\n",
    "!sacct --starttime=2024-08-01 --format=JobID,JobName,User,State,Elapsed,CPUTime,MaxRSS\n",
    "\n",
    "# Explanation:\n",
    "# --starttime=2024-08-01: Restricts the report to jobs that started on or after August 1, 2024.\n",
    "# --format: Customizes the output format to include job runtime and resource metrics like CPUTime and MaxRSS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2d048-161d-4611-b925-c05d60d29aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090213f9-1cb3-44f7-97d4-ae23d7ad8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sacct -N node2 --starttime 2024-08-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95365726-7554-4911-82dc-175307d005b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a67b47-6c93-4982-afcd-5130092f0242",
   "metadata": {},
   "source": [
    "## HPC Job Submission with Multiple Nodes in SLURM\n",
    "\n",
    "In this notebook, we will explore how to submit a job that utilizes two nodes in an HPC cluster using the SLURM workload manager. We will cover basic job submission scripts, monitoring job status, and retrieving output. We will use GROMACS, a popular molecular dynamics simulation package, to demonstrate parallel execution using MPI.\n",
    "\n",
    "### What is GROMACS?\n",
    "\n",
    "GROMACS (GROningen MAchine for Chemical Simulations) is a powerful and versatile package for molecular dynamics, primarily designed for simulating biomolecular systems such as proteins, lipids, and nucleic acids. It is capable of running on various types of computer architectures and can scale efficiently on HPC systems.\n",
    "\n",
    "#### Running GROMACS with `gmx_mpi mdrun`\n",
    "\n",
    "The `gmx_mpi mdrun` command is the main computational engine of GROMACS. It performs molecular dynamics simulations using the input files generated by pre-processing tools. The `-s` option specifies the input file containing the molecular system's topology, which is typically a `.tpr` file. For this example, we run a short simulation designed to complete quickly.\n",
    "\n",
    "#### Basic Job Submission\n",
    "\n",
    "To run a GROMACS simulation on two nodes, we need to create a SLURM batch script. This script will specify the resources required and the commands to execute.\n",
    "\n",
    "#### Creating a SLURM Job Script\n",
    "\n",
    "Below is a Python code snippet to create a SLURM script for running a GROMACS job on two nodes using MPI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3129f914-16d4-4e3c-85a8-73761032f83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM job script written to quick_gromacs_job.slurm.\n",
      "\n",
      "Contents of the SLURM job script:\n",
      "----------------------------------\n",
      "#!/bin/bash\n",
      "#SBATCH --job-name=quick_gromacs_job    # Job name\n",
      "#SBATCH --output=quick_gromacs_job.out  # Standard output\n",
      "#SBATCH --error=quick_gromacs_job.err   # Standard error\n",
      "#SBATCH --time=00:05:00                 # Time limit of 5 minutes\n",
      "#SBATCH --nodes=1                       # Number of nodes\n",
      "#SBATCH --ntasks-per-node=1             # Number of tasks per node\n",
      "#SBATCH --mem=2G                        # Allocates 1 GB of total memory per node\n",
      "\n",
      "# Load necessary modules\n",
      "module load gromacs\n",
      "\n",
      "# Run a short GROMACS simulation using MPI\n",
      "mpirun -np 2 gmx_mpi mdrun -s short_topol.tpr -nsteps 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the SLURM job script path\n",
    "slurm_script_path = \"quick_gromacs_job.slurm\"\n",
    "\n",
    "# Remove existing SLURM script if it exists\n",
    "if os.path.exists(slurm_script_path):\n",
    "    os.remove(slurm_script_path)\n",
    "\n",
    "# Create a SLURM job script for a short GROMACS run\n",
    "slurm_script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=quick_gromacs_job    # Job name\n",
    "#SBATCH --output=quick_gromacs_job.out  # Standard output\n",
    "#SBATCH --error=quick_gromacs_job.err   # Standard error\n",
    "#SBATCH --time=00:05:00                 # Time limit of 5 minutes\n",
    "#SBATCH --nodes=2                       # Number of nodes\n",
    "#SBATCH --ntasks-per-node=1             # Number of tasks per node\n",
    "#SBATCH --mem=2G                        # Allocates 1 GB of total memory per node\n",
    "\n",
    "# Load necessary modules\n",
    "module load gromacs\n",
    "\n",
    "# Run a short GROMACS simulation using MPI\n",
    "mpirun -np 2 gmx_mpi mdrun -s short_topol.tpr -nsteps 100\n",
    "\"\"\"\n",
    "\n",
    "# Write the SLURM job script to a file\n",
    "with open(slurm_script_path, \"w\") as file:\n",
    "    file.write(slurm_script)\n",
    "\n",
    "# Confirm the file has been written\n",
    "print(f\"SLURM job script written to {slurm_script_path}.\")\n",
    "\n",
    "# Make the script executable\n",
    "os.chmod(slurm_script_path, 0o755)\n",
    "\n",
    "# Read and print the contents of the SLURM job script\n",
    "with open(slurm_script_path, \"r\") as file:\n",
    "    script_content = file.read()\n",
    "\n",
    "print(\"\\nContents of the SLURM job script:\")\n",
    "print(\"----------------------------------\")\n",
    "print(script_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cbb9a47-2069-4760-9c20-e69756582a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 41\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                39 cpubase_b quick_gr  user001 PD       0:00      2 (Resources)\n",
      "                41 cpubase_b quick_gr  user001 PD       0:00      1 (Priority)\n",
      "                31 cpubase_b spawner-  user001  R      23:46      1 node1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Submit the SLURM job using the `!` syntax for direct shell command execution\n",
    "!sbatch {\"quick_gromacs_job.slurm\"}\n",
    "\n",
    "# Wait for a few seconds to ensure the job is queued\n",
    "time.sleep(2)\n",
    "\n",
    "# Check the status of the job queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdba4f95-0816-4a57-9512-61eb79426db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                39 cpubase_b quick_gr  user001 PD       0:00      2 (Resources)\n",
      "                31 cpubase_b spawner-  user001  R      23:58      1 node1\n",
      "                41 cpubase_b quick_gr  user001  R       0:01      1 node2\n"
     ]
    }
   ],
   "source": [
    "# Wait for a few seconds to ensure the job is queued\n",
    "time.sleep(5)\n",
    "\n",
    "# Check the status of the job queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64e372-e641-48dc-b537-0288682989f6",
   "metadata": {},
   "source": [
    "#### Wait until the job has finished to get the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f86106e-316f-47dd-94aa-16b62a024b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of quick_gromacs_job.out:\n",
      "----------------------------------\n",
      "\n",
      "\n",
      "Contents of quick_gromacs_job.err:\n",
      "----------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "There are not enough slots available in the system to satisfy the 2\n",
      "slots that were requested by the application:\n",
      "\n",
      "  gmx_mpi\n",
      "\n",
      "Either request fewer slots for your application, or make more slots\n",
      "available for use.\n",
      "\n",
      "A \"slot\" is the Open MPI term for an allocatable unit where we can\n",
      "launch a process.  The number of slots available are defined by the\n",
      "environment in which Open MPI processes are run:\n",
      "\n",
      "  1. Hostfile, via \"slots=N\" clauses (N defaults to number of\n",
      "     processor cores if not provided)\n",
      "  2. The --host command line parameter, via a \":N\" suffix on the\n",
      "     hostname (N defaults to 1 if not provided)\n",
      "  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)\n",
      "  4. If none of a hostfile, the --host command line parameter, or an\n",
      "     RM is present, Open MPI defaults to the number of processor cores\n",
      "\n",
      "In all the above cases, if you want Open MPI to default to the number\n",
      "of hardware threads instead of the number of processor cores, use the\n",
      "--use-hwthread-cpus option.\n",
      "\n",
      "Alternatively, you can use the --oversubscribe option to ignore the\n",
      "number of available slots when deciding the number of processes to\n",
      "launch.\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths to the output and error files\n",
    "output_file = \"quick_gromacs_job.out\"\n",
    "error_file = \"quick_gromacs_job.err\"\n",
    "\n",
    "# Check and display the contents of the output file\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"\\nContents of {output_file}:\")\n",
    "    print(\"----------------------------------\")\n",
    "    with open(output_file, \"r\") as file:\n",
    "        output_content = file.read()\n",
    "        print(output_content)\n",
    "else:\n",
    "    print(f\"\\n{output_file} does not exist.\")\n",
    "\n",
    "# Check and display the contents of the error file\n",
    "if os.path.exists(error_file):\n",
    "    print(f\"\\nContents of {error_file}:\")\n",
    "    print(\"----------------------------------\")\n",
    "    with open(error_file, \"r\") as file:\n",
    "        error_content = file.read()\n",
    "        print(error_content)\n",
    "else:\n",
    "    print(f\"\\n{error_file} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c358f4-1891-48ca-a8b1-093fcc6048ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4364f6-e92b-4499-8024-db240b2c7123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
