{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8RhFcvahFkU"
   },
   "source": [
    "# Performance Optimization in HPC\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore the fundamental techniques for optimizing code performance in High-Performance Computing (HPC) environments. Performance optimization is crucial for fully exploiting the capabilities of HPC architectures. By understanding and applying these techniques, you can significantly reduce the runtime of your computational tasks, making them more efficient and scalable.\n",
    "\n",
    "This practice is essential in HPC as it allows for better resource utilization, reduced costs, and the ability to solve larger and more complex problems. We will cover various optimization strategies, including code profiling, memory hierarchy optimization, and the use of high-performance libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fcZ7APohM10"
   },
   "source": [
    "## 2. Optimizing Code for HPC Architectures\n",
    "\n",
    "### 2.1 Code Profiling and Analysis\n",
    "\n",
    "Before optimizing any code, it's essential to understand where the bottlenecks are. Profiling tools help identify the most time-consuming parts of your code, which are the primary candidates for optimization.\n",
    "\n",
    "### 2.2 Loop Unrolling and Vectorization\n",
    "\n",
    "Loop unrolling and vectorization are common techniques used to enhance the performance of loops, which are often the most time-consuming parts of computational code.\n",
    "\n",
    "### 2.3 Memory Access Patterns and Cache Utilization\n",
    "\n",
    "Efficient memory access patterns and effective use of the CPU cache can dramatically speed up your programs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 958,
     "status": "ok",
     "timestamp": 1724421332715,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "BGyzMXXnftjM",
    "outputId": "073e3593-21f0-490c-971d-22a8dd69161c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         7 function calls in 0.027 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.027    0.027 <__array_function__ internals>:177(dot)\n",
      "        1    0.000    0.000    0.027    0.027 <ipython-input-6-cda063ce1d20>:6(matrix_multiply)\n",
      "        1    0.000    0.000    0.027    0.027 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 multiarray.py:740(dot)\n",
      "        1    0.000    0.000    0.027    0.027 {built-in method builtins.exec}\n",
      "        1    0.027    0.027    0.027    0.027 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Profiling a simple matrix multiplication function using cProfile\n",
    "\n",
    "import cProfile\n",
    "import numpy as np\n",
    "\n",
    "def matrix_multiply(A, B):\n",
    "    return np.dot(A, B)\n",
    "\n",
    "# Create large random matrices\n",
    "A = np.random.rand(1000, 1000)\n",
    "B = np.random.rand(1000, 1000)\n",
    "\n",
    "# Profile the matrix multiplication\n",
    "cProfile.run('matrix_multiply(A, B)')\n",
    "\n",
    "# The output will show where the time is being spent in the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_l2qcwJhX9N"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "The above code uses Python's `cProfile` to profile a matrix multiplication function. Profiling helps identify the parts of the code that consume the most computational resources, allowing us to focus our optimization efforts effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJXcLqLChcOc"
   },
   "source": [
    "## 3. Memory Hierarchy and Data Locality\n",
    "\n",
    "### 3.1 Understanding Memory Hierarchy\n",
    "\n",
    "Memory hierarchy, from registers to cache and RAM, plays a critical role in the performance of HPC applications. Optimizing for memory hierarchy can significantly reduce data access times.\n",
    "\n",
    "### 3.2 Data Locality\n",
    "\n",
    "Data locality refers to the use of data elements within close proximity in memory, reducing cache misses and improving overall performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25777,
     "status": "ok",
     "timestamp": 1724421406179,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "afJj6SmghY-E",
    "outputId": "219df990-271e-4aa9-f737-368e8322b8fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row-wise sum time: 7.568900108337402\n",
      "Column-wise sum time: 9.489133596420288\n"
     ]
    }
   ],
   "source": [
    "# Example: Measuring the impact of data locality on performance\n",
    "\n",
    "import time\n",
    "\n",
    "def sum_rows(matrix):\n",
    "    total = 0\n",
    "    for row in matrix:\n",
    "        total += sum(row)\n",
    "    return total\n",
    "\n",
    "def sum_columns(matrix):\n",
    "    total = 0\n",
    "    for col in range(matrix.shape[1]):\n",
    "        total += sum(matrix[:, col])\n",
    "    return total\n",
    "\n",
    "# Create a large matrix\n",
    "matrix = np.random.rand(10000, 10000)\n",
    "\n",
    "# Measure row-wise sum performance\n",
    "start_time = time.time()\n",
    "sum_rows(matrix)\n",
    "print(\"Row-wise sum time:\", time.time() - start_time)\n",
    "\n",
    "# Measure column-wise sum performance\n",
    "start_time = time.time()\n",
    "sum_columns(matrix)\n",
    "print(\"Column-wise sum time:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcSAxdv4hf1c"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "In the above example, we measure the performance impact of accessing matrix elements row-wise versus column-wise. Due to the way memory is structured, row-wise access is typically faster because it accesses contiguous memory locations, which is more cache-friendly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsjf9ryRhqJF"
   },
   "source": [
    "## 4. High-Performance Libraries for Scientific Computing\n",
    "\n",
    "Leveraging high-performance libraries can save development time and ensure that your code is optimized for modern HPC architectures.\n",
    "\n",
    "### 4.1 Using BLAS and LAPACK for Linear Algebra\n",
    "\n",
    "BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) are standard libraries that provide optimized implementations of basic linear algebra routines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1724421430380,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "ZyvxXyG4hsA0",
    "outputId": "61ab16a4-78d3-4ba1-9667-b5ce15174e6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[5.95337697e-01 4.85282673e-01 9.04776304e-01]\n",
      " [6.50926963e-01 6.98687847e-04 8.10228640e-02]\n",
      " [7.12702788e-01 2.33867411e-01 9.94630157e-01]]\n",
      "Matrix B:\n",
      " [[0.4799877  0.32479143 0.65512874]\n",
      " [0.37861658 0.91921534 0.29831936]\n",
      " [0.62257541 0.94410101 0.25187856]]\n",
      "Resulting matrix C (A * B):\n",
      " [[1.03278232 1.49364008 0.7626858 ]\n",
      " [0.36314431 0.28855151 0.44705731]\n",
      " [1.04986693 1.38548561 0.78720527]]\n",
      "Resulting matrix shape: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check if NumPy and SciPy are installed, and install if not\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Try importing necessary packages and install if not available\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    install('numpy')\n",
    "    import numpy as np\n",
    "\n",
    "try:\n",
    "    from scipy.linalg import blas\n",
    "except ImportError:\n",
    "    install('scipy')\n",
    "    from scipy.linalg import blas\n",
    "\n",
    "# Now perform the matrix multiplication using BLAS\n",
    "\n",
    "# Create random matrices A and B\n",
    "A = np.random.rand(3, 3)\n",
    "B = np.random.rand(3, 3)\n",
    "\n",
    "# Ensure matrices are in Fortran-contiguous order\n",
    "A_f = np.asfortranarray(A)\n",
    "B_f = np.asfortranarray(B)\n",
    "\n",
    "# Using BLAS dgemm for matrix multiplication\n",
    "C = blas.dgemm(1.0, A_f, B_f)\n",
    "\n",
    "# Print the result\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Matrix B:\\n\", B)\n",
    "print(\"Resulting matrix C (A * B):\\n\", C)\n",
    "print(\"Resulting matrix shape:\", C.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqcoRrtWhxzM"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "Here, we use the `dgemm` function from BLAS, accessed via SciPy, to perform matrix multiplication. This function is highly optimized for performance on many HPC systems, often outperforming custom implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bew1446Ah2PF"
   },
   "source": [
    "# 5 Checkpointing in High-Performance Computing (HPC)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Checkpointing is an essential technique in HPC to ensure that long-running computations can recover from failures without having to start over from the beginning. It involves periodically saving the state of an application so that it can be resumed from the last checkpoint.\n",
    "\n",
    "In this notebook, we'll simulate a checkpointing mechanism using Python. We'll periodically save the state of a computation to a file, and if the program is interrupted, it can resume from the last saved state.\n",
    "\n",
    "### Part 1: Setting Up the Environment\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up the checkpointing mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_checkpoint(data, checkpoint_file='checkpoint.pkl'):\n",
    "    \"\"\"Save the current state to a checkpoint file.\"\"\"\n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Checkpoint saved: {data}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_file='checkpoint.pkl'):\n",
    "    \"\"\"Load the state from a checkpoint file.\"\"\"\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Checkpoint loaded: {data}\")\n",
    "        return data\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Simulating a Computation with Checkpointing\n",
    "\n",
    "We'll simulate a long-running computation, such as a loop that performs some calculations. We'll save the state of the computation at regular intervals (checkpoints). If the program stops, it can resume from the last checkpoint.\n",
    "\n",
    "### Steps:\n",
    "1. **Run a computation** and save the state every few iterations.\n",
    "2. **Simulate an interruption** by stopping the program.\n",
    "3. **Resume the computation** from the last checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing iteration 1\n",
      "Computing iteration 2\n",
      "Computing iteration 3\n",
      "Computing iteration 4\n",
      "Computing iteration 5\n",
      "Checkpoint saved: 5\n",
      "Computing iteration 6\n",
      "Computing iteration 7\n",
      "Computing iteration 8\n",
      "Computing iteration 9\n",
      "Computing iteration 10\n",
      "Checkpoint saved: 10\n",
      "Computing iteration 11\n",
      "Computing iteration 12\n",
      "Computing iteration 13\n",
      "Computing iteration 14\n",
      "Computing iteration 15\n",
      "Checkpoint saved: 15\n",
      "Computing iteration 16\n",
      "Computing iteration 17\n",
      "Computing iteration 18\n",
      "Computing iteration 19\n",
      "Computation completed.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def perform_computation(start, end, checkpoint_file='checkpoint.pkl'):\n",
    "    \"\"\"Perform a computation with checkpointing.\"\"\"\n",
    "    for i in range(start, end):\n",
    "        # Simulate some computation\n",
    "        print(f\"Computing iteration {i}\")\n",
    "        time.sleep(0.5)  # Simulate time-consuming work\n",
    "        \n",
    "        # Save checkpoint every 5 iterations\n",
    "        if i % 5 == 0:\n",
    "            save_checkpoint(i, checkpoint_file)\n",
    "    \n",
    "    print(\"Computation completed.\")\n",
    "\n",
    "# Load the last checkpoint if it exists\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "# Start from the checkpoint or from the beginning\n",
    "start_iteration = checkpoint + 1 if checkpoint is not None else 1\n",
    "end_iteration = 20\n",
    "\n",
    "# Perform the computation with checkpointing\n",
    "perform_computation(start_iteration, end_iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- **Checkpointing Mechanism**: The `save_checkpoint` function saves the current iteration number to a file. This simulates saving the state of a computation. The `load_checkpoint` function checks if a checkpoint file exists and loads the last saved state.\n",
    "  \n",
    "- **Computation**: The computation is simulated by a loop that prints the current iteration and sleeps for half a second to simulate work. Every 5 iterations, the state is saved to the checkpoint file.\n",
    "\n",
    "- **Resumption**: If the program is interrupted and restarted, it will resume from the last saved iteration, rather than starting from the beginning.\n",
    "\n",
    "### Optional Exercise:\n",
    "- **Simulate an Interruption**: After a few iterations, stop the notebook and restart it. Observe how the program resumes from the last saved checkpoint.\n",
    "- **Modify the Checkpoint Interval**: Change the checkpoint interval from every 5 iterations to every 2 or 10 iterations and see how it affects the resumption.\n",
    "- **Extend the Computation**: Increase the total number of iterations and observe how checkpointing helps in resuming the long-running computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Simulating an Error and Recovery\n",
    "\n",
    "In this section, we'll simulate an error during the computation to see how checkpointing can help recover from the failure. We'll introduce an intentional error in the computation, stop the program, and then restart it to resume from the last saved checkpoint.\n",
    "\n",
    "### Steps:\n",
    "1. **Introduce an error** in the computation.\n",
    "2. **Simulate the recovery** by catching the error and re-running the program.\n",
    "3. **Observe the recovery** process as the program resumes from the last checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded: 15\n",
      "Computing iteration 16\n",
      "Computing iteration 17\n",
      "Computing iteration 18\n",
      "Computing iteration 19\n",
      "Computation completed.\n"
     ]
    }
   ],
   "source": [
    "def perform_computation_with_error(start, end, checkpoint_file='checkpoint.pkl'):\n",
    "    \"\"\"Perform a computation with an intentional error and checkpointing.\"\"\"\n",
    "    for i in range(start, end):\n",
    "        # Simulate some computation\n",
    "        print(f\"Computing iteration {i}\")\n",
    "        time.sleep(0.5)  # Simulate time-consuming work\n",
    "\n",
    "        # Introduce an error at iteration 10\n",
    "        if i == 10:\n",
    "            raise RuntimeError(f\"Error encountered at iteration {i}!\")\n",
    "\n",
    "        # Save checkpoint every 5 iterations\n",
    "        if i % 5 == 0:\n",
    "            save_checkpoint(i, checkpoint_file)\n",
    "    \n",
    "    print(\"Computation completed.\")\n",
    "\n",
    "# Load the last checkpoint if it exists\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "# Start from the checkpoint or from the beginning\n",
    "start_iteration = checkpoint + 1 if checkpoint is not None else 1\n",
    "end_iteration = 20\n",
    "\n",
    "try:\n",
    "    # Perform the computation with an intentional error and checkpointing\n",
    "    perform_computation_with_error(start_iteration, end_iteration)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "    print(\"An error occurred. Attempting to recover from the last checkpoint...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- **Error Simulation**: An intentional error is introduced at iteration 10 by raising a `RuntimeError`. This simulates an unexpected failure in the computation.\n",
    "  \n",
    "- **Error Handling**: The error is caught by a `try-except` block, which prints an error message and attempts to recover by restarting the computation from the last checkpoint.\n",
    "\n",
    "- **Recovery Process**: After the error is caught, the program can be restarted, and it will automatically resume from the last checkpoint (iteration 10 in this case).\n",
    "\n",
    "### Exercise:\n",
    "- **Run the Program**: Execute the code to see how the error is handled. Observe how the program resumes from the last checkpoint after the error.\n",
    "- **Change the Error Condition**: Modify the code to introduce the error at a different iteration and observe how the checkpointing mechanism adjusts.\n",
    "- **Expand the Recovery Mechanism**: Enhance the recovery process by automatically restarting the computation after an error without requiring manual intervention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded: 15\n",
      "Computing iteration 16\n",
      "Computing iteration 17\n",
      "Computing iteration 18\n",
      "Computing iteration 19\n",
      "Computation completed.\n"
     ]
    }
   ],
   "source": [
    "# Load the last checkpoint again (to simulate restarting the program)\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "# Start from the checkpoint or from the beginning\n",
    "start_iteration = checkpoint + 1 if checkpoint is not None else 1\n",
    "end_iteration = 20\n",
    "\n",
    "# Re-run the computation to recover from the error\n",
    "perform_computation(start_iteration, end_iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we've demonstrated how checkpointing can help recover from errors in a long-running computation. By periodically saving the state of the computation, we were able to resume from the last checkpoint after encountering an error, minimizing the loss of progress.\n",
    "\n",
    "This technique is crucial in HPC environments, where computations can run for extended periods, and unexpected failures can result in significant loss of work. By implementing checkpointing, we can ensure that our computations are more resilient to such failures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9v6MP3Hob1D"
   },
   "source": [
    "## 6. Introduction to Performance Tuning and Analysis\n",
    "\n",
    "Performance tuning and analysis are crucial for maximizing the efficiency of HPC applications. This section introduces the fundamental steps involved in performance tuning, including identifying bottlenecks, applying optimizations, and verifying improvements.\n",
    "\n",
    "### 6.1 Overview of Performance Tuning Steps\n",
    "\n",
    "The general workflow for performance tuning involves:\n",
    "1. **Profiling the code** to identify performance bottlenecks.\n",
    "2. **Applying optimizations** to the identified bottlenecks.\n",
    "3. **Reprofiling the code** to assess the impact of the optimizations.\n",
    "4. **Iterating** until performance goals are met.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8xwNdtYpH3q"
   },
   "source": [
    "## 7. Profiling and Identifying Bottlenecks\n",
    "\n",
    "In this section, we'll profile a computational code to identify the most time-consuming parts. Profiling is the first step in any performance tuning process.\n",
    "\n",
    "### 7.1 Profiling with cProfile and line_profiler\n",
    "\n",
    "We'll use `cProfile` for an overall view of the code's performance and `line_profiler` for detailed line-by-line analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2073,
     "status": "ok",
     "timestamp": 1724423697165,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "pPw0GkixpG0i",
    "outputId": "19b4669a-940a-46e3-e99b-74b545e51218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         36 function calls in 0.767 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.054    0.054 <__array_function__ internals>:177(dot)\n",
      "        1    0.000    0.000    0.706    0.706 <__array_function__ internals>:177(inv)\n",
      "        1    0.000    0.000    0.007    0.007 <__array_function__ internals>:177(sum)\n",
      "        1    0.000    0.000    0.767    0.767 <ipython-input-14-fb907a52bf7a>:4(compute_heavy_task)\n",
      "        1    0.000    0.000    0.767    0.767 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2183(_sum_dispatcher)\n",
      "        1    0.000    0.000    0.007    0.007 fromnumeric.py:2188(sum)\n",
      "        1    0.000    0.000    0.007    0.007 fromnumeric.py:69(_wrapreduction)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 linalg.py:107(get_linalg_error_extobj)\n",
      "        1    0.000    0.000    0.000    0.000 linalg.py:112(_makearray)\n",
      "        2    0.000    0.000    0.000    0.000 linalg.py:117(isComplexType)\n",
      "        1    0.000    0.000    0.000    0.000 linalg.py:130(_realType)\n",
      "        1    0.000    0.000    0.000    0.000 linalg.py:136(_commonType)\n",
      "        1    0.000    0.000    0.000    0.000 linalg.py:180(_assert_stacked_2d)\n",
      "        1    0.000    0.000    0.000    0.000 linalg.py:186(_assert_stacked_square)\n",
      "        1    0.000    0.000    0.000    0.000 linalg.py:465(_unary_dispatcher)\n",
      "        1    0.706    0.706    0.706    0.706 linalg.py:469(inv)\n",
      "        1    0.000    0.000    0.000    0.000 multiarray.py:740(dot)\n",
      "        1    0.000    0.000    0.767    0.767 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
      "        3    0.054    0.018    0.767    0.256 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__array_prepare__' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.007    0.007    0.007    0.007 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.450197 s\n",
       "File: <ipython-input-14-fb907a52bf7a>\n",
       "Function: compute_heavy_task at line 4\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     4                                           def compute_heavy_task(A, B):\n",
       "     5         1   22649567.0    2e+07      5.0      C = np.dot(A, B)\n",
       "     6         1  426114418.0    4e+08     94.7      D = np.linalg.inv(C)\n",
       "     7         1    1432723.0    1e+06      0.3      E = np.sum(D)\n",
       "     8         1        700.0    700.0      0.0      return E"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cProfile\n",
    "import numpy as np\n",
    "\n",
    "def compute_heavy_task(A, B):\n",
    "    C = np.dot(A, B)\n",
    "    D = np.linalg.inv(C)\n",
    "    E = np.sum(D)\n",
    "    return E\n",
    "\n",
    "# Create large random matrices\n",
    "A = np.random.rand(1000, 1000)\n",
    "B = np.random.rand(1000, 1000)\n",
    "\n",
    "# Profile the function\n",
    "cProfile.run('compute_heavy_task(A, B)')\n",
    "\n",
    "# Detailed line profiling\n",
    "%lprun -f compute_heavy_task compute_heavy_task(A, B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgKcVao9pLIC"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "The code above uses `cProfile` to profile the entire function and `line_profiler` for a detailed line-by-line breakdown. This helps in identifying which parts of the code are the most time-consuming.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Try modifying the `compute_heavy_task` function by adding other operations, such as matrix transposition or element-wise multiplication. Re-run the profiling tools to see how the performance characteristics change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vds-N48VpMqS"
   },
   "source": [
    "## 8. Applying Optimizations\n",
    "\n",
    "Once bottlenecks are identified, the next step is to apply optimizations. In this section, we will optimize matrix operations using techniques such as loop unrolling, vectorization, and memory access optimization.\n",
    "\n",
    "### 8.1 Loop Unrolling and Vectorization\n",
    "\n",
    "We will revisit loop unrolling and vectorization to see how they can improve performance in matrix operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31629,
     "status": "ok",
     "timestamp": 1724423753151,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "dhh0RdPMpN-7",
    "outputId": "b9ed8497-4cf8-425c-84cd-571a70f08310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic matrix sum time: 17.063485383987427\n",
      "Vectorized matrix sum time: 0.06578946113586426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def basic_matrix_sum(matrix):\n",
    "    total = 0\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            total += matrix[i, j]\n",
    "    return total\n",
    "\n",
    "def vectorized_matrix_sum(matrix):\n",
    "    return np.sum(matrix)\n",
    "\n",
    "# Create a large matrix\n",
    "matrix = np.random.rand(10000, 10000)\n",
    "\n",
    "# Measure time for basic matrix sum\n",
    "start_time = time.time()\n",
    "basic_sum = basic_matrix_sum(matrix)\n",
    "print(\"Basic matrix sum time:\", time.time() - start_time)\n",
    "\n",
    "# Measure time for vectorized matrix sum\n",
    "start_time = time.time()\n",
    "vectorized_sum = vectorized_matrix_sum(matrix)\n",
    "print(\"Vectorized matrix sum time:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvuGwJtopPWB"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "This example compares the performance of a basic loop-based matrix sum with a vectorized version using NumPy's built-in `sum` function. Vectorization allows for faster computation by leveraging SIMD instructions.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Try optimizing the `basic_matrix_sum` function by manually unrolling the loops. Measure the performance impact and compare it with the vectorized approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAmDUSMPpRma"
   },
   "source": [
    "## 9. Memory Access Optimization and Cache Utilization\n",
    "\n",
    "Memory access patterns greatly affect the performance of HPC applications. In this section, we'll explore techniques to optimize memory access and improve cache utilization.\n",
    "\n",
    "### 9.1 Optimizing Memory Access Patterns\n",
    "\n",
    "Efficient memory access patterns reduce cache misses, leading to faster execution times. We'll analyze the impact of row-major vs. column-major access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66767,
     "status": "ok",
     "timestamp": 1724423826332,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "EvLNYS3OpQkT",
    "outputId": "4ce058a5-24b6-4f1b-ab9f-5e0c97b6ad3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row-major sum time: 16.830533266067505\n",
      "Column-major sum time: 18.51984715461731\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def row_major_sum(matrix):\n",
    "    total = 0\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            total += matrix[i, j]\n",
    "    return total\n",
    "\n",
    "def column_major_sum(matrix):\n",
    "    total = 0\n",
    "    for j in range(matrix.shape[1]):\n",
    "        for i in range(matrix.shape[0]):\n",
    "            total += matrix[i, j]\n",
    "    return total\n",
    "\n",
    "# Create a large matrix\n",
    "matrix = np.random.rand(10000, 10000)\n",
    "\n",
    "# Measure row-major access time\n",
    "start_time = time.time()\n",
    "row_sum = row_major_sum(matrix)\n",
    "print(\"Row-major sum time:\", time.time() - start_time)\n",
    "\n",
    "# Measure column-major access time\n",
    "start_time = time.time()\n",
    "column_sum = column_major_sum(matrix)\n",
    "print(\"Column-major sum time:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbkdDKQjpUh6"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "The example above compares row-major and column-major memory access patterns. Typically, row-major access is faster on most systems because it aligns better with how data is stored in memory.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Modify the code to measure the cache hit rate (if possible using advanced profiling tools or libraries) for each access pattern. Observe how different matrix sizes affect cache utilization and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weiiRf7ppWCp"
   },
   "source": [
    "## 10. Leveraging High-Performance Libraries\n",
    "\n",
    "Using specialized HPC libraries can significantly enhance the performance of your applications. This section explores how to use BLAS, LAPACK, and other optimized libraries in your code.\n",
    "\n",
    "### 10.1 Using BLAS and LAPACK for Matrix Operations\n",
    "\n",
    "BLAS (Basic Linear Algebra Subprograms) and LAPACK are standard libraries providing highly optimized implementations of basic linear algebra routines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1724424101113,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "tek106G0pXGC",
    "outputId": "c05ef701-d960-47a5-da21-27c1cb53ea80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[0.66219264 0.93976229 0.80629067]\n",
      " [0.21218842 0.6871569  0.00316694]\n",
      " [0.14518797 0.40285409 0.98706225]]\n",
      "\n",
      "Matrix B:\n",
      "[[0.78814114 0.18843332 0.60795389]\n",
      " [0.27090502 0.75505722 0.6624193 ]\n",
      " [0.23484097 0.39099746 0.84414894]]\n",
      "\n",
      "Result of BLAS matrix multiplication (A * B = C):\n",
      "[[0.96583766 1.14961107 1.70572868]\n",
      " [0.3541324  0.56006441 0.58686014]\n",
      " [0.45536646 0.71747498 1.18835346]]\n",
      "\n",
      "Matrix inversion of A using LAPACK:\n",
      "[[ 2.81627704 -2.5075871  -2.29245574]\n",
      " [-0.86936937  2.23209269  0.70299062]\n",
      " [-0.05942942 -0.5421504   1.06339225]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import blas, lapack\n",
    "\n",
    "# Create large random matrices\n",
    "A = np.random.rand(3, 3)  # Using smaller matrices for easier visualization\n",
    "B = np.random.rand(3, 3)\n",
    "\n",
    "# Using BLAS dgemm for matrix multiplication\n",
    "C = blas.dgemm(1.0, A, B)\n",
    "\n",
    "# Using LAPACK for matrix inversion (getrf followed by getri)\n",
    "LU, piv, info = lapack.dgetrf(A)\n",
    "inv_matrix, info = lapack.dgetri(LU, piv)\n",
    "\n",
    "# Display the results\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "print(\"\\nResult of BLAS matrix multiplication (A * B = C):\")\n",
    "print(C)\n",
    "\n",
    "print(\"\\nMatrix inversion of A using LAPACK:\")\n",
    "print(inv_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30OdE7-6pZgR"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "This example demonstrates how to use the BLAS `dgemm` function for matrix multiplication and the LAPACK `dgetrf` function for matrix inversion. These libraries are optimized for performance on many HPC systems.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Try using other functions from BLAS and LAPACK, such as `dsymv` for symmetric matrix-vector multiplication or `dgeev` for eigenvalue computation. Compare the performance of these library functions with your custom implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vHHfgbkpc0B"
   },
   "source": [
    "## 11. Advanced Performance Tuning with Parallel I/O\n",
    "\n",
    "Efficient I/O operations are critical for handling large datasets in HPC applications. This section covers advanced parallel I/O techniques using mpi4py.\n",
    "\n",
    "### 11.1 Implementing Parallel I/O\n",
    "\n",
    "We will extend our previous examples by implementing collective I/O operations, which can be more efficient for large-scale data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1724424207375,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "vZUOI0Fupbrs",
    "outputId": "e1526f35-a6b7-46d0-9021-4529f429fdd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0: First element = 0, Last element = 0\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Create a large array on each process\n",
    "data = np.full(1000000, rank, dtype='i')\n",
    "\n",
    "# Write data collectively to a shared file\n",
    "fh = MPI.File.Open(comm, 'collective_output.dat', MPI.MODE_CREATE | MPI.MODE_WRONLY)\n",
    "fh.Write_at_all(rank * data.nbytes, data)\n",
    "fh.Close()  # Manually close the file\n",
    "\n",
    "# Reading data collectively\n",
    "collected_data = np.empty_like(data)\n",
    "fh = MPI.File.Open(comm, 'collective_output.dat', MPI.MODE_RDONLY)\n",
    "fh.Read_at_all(rank * collected_data.nbytes, collected_data)\n",
    "fh.Close()  # Manually close the file after reading\n",
    "\n",
    "# Print out a summary of the data to verify the read operation\n",
    "print(f\"Process {rank}: First element = {collected_data[0]}, Last element = {collected_data[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gcsx5P71pflx"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "In this example, each MPI process writes and reads a portion of data from a shared file using collective I/O operations. This technique improves the efficiency of data handling in parallel applications.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Modify the code to test the performance impact of different file access modes, such as `MPI.MODE_APPEND` or non-collective I/O. Analyze how these changes affect the scalability of I/O operations when running on multiple processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXmUJRN9pima"
   },
   "source": [
    "## 12. Comprehensive Performance Analysis and Tuning\n",
    "\n",
    "In this section, we will perform a comprehensive performance analysis and tuning of a complex HPC application. We will use profiling tools to identify bottlenecks and optimize the application.\n",
    "\n",
    "### 12.1 Case Study: Performance Tuning of a Scientific Application\n",
    "\n",
    "We will apply profiling, optimization, and parallel I/O techniques to a real-world scientific computation. The code will include matrix operations and parallel I/O.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1724424550186,
     "user": {
      "displayName": "Oscar Diez",
      "userId": "09940749782853693007"
     },
     "user_tz": -120
    },
    "id": "18VZBQ0IpkKL",
    "outputId": "5f17a46a-56fa-4949-ee7b-979da91b4892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         13 function calls in 0.231 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.001    0.001 <__array_function__ internals>:177(sum)\n",
      "        1    0.231    0.231    0.231    0.231 <ipython-input-19-f856f0057939>:20(optimized_computation)\n",
      "        1    0.000    0.000    0.231    0.231 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2183(_sum_dispatcher)\n",
      "        1    0.000    0.000    0.001    0.001 fromnumeric.py:2188(sum)\n",
      "        1    0.000    0.000    0.001    0.001 fromnumeric.py:69(_wrapreduction)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
      "        1    0.000    0.000    0.231    0.231 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.001    0.001 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "\n",
      "\n",
      "Process 0 completed its task and saved the result. Result sum: -0.03398592324315608\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import blas, lapack\n",
    "from mpi4py import MPI\n",
    "import cProfile\n",
    "import time\n",
    "\n",
    "# MPI setup\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Problem size (reduced for better performance)\n",
    "N = 500  # Smaller matrix size for quicker computation\n",
    "\n",
    "# Create large random matrices\n",
    "A = np.random.rand(N, N)\n",
    "B = np.random.rand(N, N)\n",
    "\n",
    "# Optimized computation\n",
    "def optimized_computation(A, B):\n",
    "    C = blas.dgemm(1.0, A, B)\n",
    "    LU, piv, info = lapack.dgetrf(C)\n",
    "    inv_matrix, info = lapack.dgetri(LU, piv)\n",
    "    result = np.sum(inv_matrix)\n",
    "    return result\n",
    "\n",
    "# Profile the optimized computation\n",
    "cProfile.run('optimized_computation(A, B)')\n",
    "\n",
    "# Perform the computation\n",
    "result = optimized_computation(A, B)\n",
    "\n",
    "# Parallel I/O to save results\n",
    "file_handle = MPI.File.Open(comm, 'final_result.dat', MPI.MODE_CREATE | MPI.MODE_WRONLY)\n",
    "result_array = np.array([result], dtype='d')\n",
    "file_handle.Write_at_all(rank * result_array.nbytes, result_array)\n",
    "file_handle.Close()\n",
    "\n",
    "# Final verification\n",
    "print(f\"Process {rank} completed its task and saved the result. Result sum: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiTwilzjpm6q"
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "This case study brings together various optimization and parallelization techniques to solve a large-scale matrix problem. The code includes profiling, the use of high-performance libraries, and parallel I/O for saving the results.\n",
    "\n",
    "### Optional Exercise:\n",
    "\n",
    "Expand the case study by adding more complex operations, such as eigenvalue computation or solving a system of linear equations. Profile and optimize these additional steps, and analyze how the performance scales with the problem size and number of processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Array Summation with MPI\n",
    "\n",
    "In this notebook, we will explore how to implement a parallel array summation using MPI in C. We'll start with a basic implementation and then introduce an optimization to improve performance.\n",
    "\n",
    "This example will guide you through:\n",
    "1. Writing a simple C program for parallel summation using MPI.\n",
    "2. Compiling and running the program in a Jupyter notebook.\n",
    "3. Implementing an optimization using OpenMP.\n",
    "4. Comparing the performance of the original and optimized versions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Write the Initial C Program\n",
    "\n",
    "We'll start by writing a simple C program that sums an array in parallel using MPI. The program initializes an array, distributes it across multiple processes, and then each process computes the sum of its portion. Finally, the root process collects the sums and computes the total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C program with file output written to mpi_sum_file_output.c\n"
     ]
    }
   ],
   "source": [
    "c_program_with_file_output = \"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "    int n = 100000;  // Size of the array\n",
    "    int *array = NULL;\n",
    "    int local_sum = 0;\n",
    "\n",
    "    if (rank == 0) {\n",
    "        array = (int*)malloc(n * sizeof(int));\n",
    "        for (int i = 0; i < n; i++) {\n",
    "            array[i] = 1;  // Initialize array with ones\n",
    "        }\n",
    "    }\n",
    "\n",
    "    int local_n = n / size;\n",
    "    int *local_array = (int*)malloc(local_n * sizeof(int));\n",
    "\n",
    "    MPI_Scatter(array, local_n, MPI_INT, local_array, local_n, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "    for (int i = 0; i < local_n; i++) {\n",
    "        local_sum += local_array[i];\n",
    "    }\n",
    "\n",
    "    int total_sum = 0;\n",
    "    MPI_Reduce(&local_sum, &total_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "    if (rank == 0) {\n",
    "        printf(\"Total sum = %d\\\\n\", total_sum);\n",
    "        free(array);\n",
    "    }\n",
    "\n",
    "    free(local_array);\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Save the C program to a file\n",
    "with open(\"mpi_sum_file_output.c\", \"w\") as file:\n",
    "    file.write(c_program_with_file_output)\n",
    "\n",
    "print(\"C program with file output written to mpi_sum_file_output.c\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compile and Run the Initial Program\n",
    "\n",
    "Now that we've written our initial C program, let's compile it using `mpicc` and run it with `mpirun`. We'll use 4 processes to demonstrate the parallel execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation successful.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Compile the C program\n",
    "compile_process = subprocess.run([\"mpicc\", \"-o\", \"mpi_sum_file_output\", \"mpi_sum_file_output.c\"], capture_output=True, text=True)\n",
    "\n",
    "# Check if compilation was successful\n",
    "if compile_process.returncode == 0:\n",
    "    print(\"Compilation successful.\")\n",
    "else:\n",
    "    print(\"Compilation failed:\")\n",
    "    print(compile_process.stderr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the Slurm sbatch file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM batch script written to mpi_sum_job.slurm\n"
     ]
    }
   ],
   "source": [
    "slurm_script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=mpi_sum_job\n",
    "#SBATCH --output=slurm_output.txt\n",
    "#SBATCH --ntasks=2\n",
    "#SBATCH --time=00:05:00\n",
    "#SBATCH --partition=slurmpar\n",
    "\n",
    "mpirun ./mpi_sum_file_output\n",
    "\"\"\"\n",
    "\n",
    "# Save the SLURM script to a file\n",
    "with open(\"mpi_sum_job.slurm\", \"w\") as file:\n",
    "    file.write(slurm_script)\n",
    "\n",
    "print(\"SLURM batch script written to mpi_sum_job.slurm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the slurm job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM job submitted successfully.\n",
      "Submitted batch job 4\n",
      "\n",
      "SLURM Job Output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Submit the SLURM job\n",
    "submit_process = subprocess.run([\"sbatch\", \"mpi_sum_job.slurm\"], capture_output=True, text=True)\n",
    "\n",
    "# Check if submission was successful\n",
    "if submit_process.returncode == 0:\n",
    "    print(\"SLURM job submitted successfully.\")\n",
    "    print(submit_process.stdout)\n",
    "else:\n",
    "    print(\"Failed to submit SLURM job:\")\n",
    "    print(submit_process.stderr)\n",
    "\n",
    "# Wait a bit for the job to complete and then read the output\n",
    "import time\n",
    "time.sleep(5)  # Adjust the sleep time depending on how long the job might take\n",
    "\n",
    "# Read and print the contents of the SLURM output file\n",
    "try:\n",
    "    with open(\"slurm_output.txt\", \"r\") as f:\n",
    "        output = f.read()\n",
    "    print(\"SLURM Job Output:\")\n",
    "    print(output)\n",
    "except FileNotFoundError:\n",
    "    print(\"SLURM output file not found. The job may not have run correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Optimize the Program\n",
    "\n",
    "We'll now optimize our program using OpenMP to parallelize the summation within each MPI process. This can lead to significant performance improvements, especially on multi-core systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized C program written to mpi_sum_optimized.c\n"
     ]
    }
   ],
   "source": [
    "c_program_optimized = \"\"\"\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <omp.h>  // Include OpenMP for parallel reduction\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "    int n = 1000000;\n",
    "    int *array = NULL;\n",
    "    int local_sum = 0;\n",
    "\n",
    "    if (rank == 0) {\n",
    "        array = (int*)malloc(n * sizeof(int));\n",
    "        for (int i = 0; i < n; i++) {\n",
    "            array[i] = 1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    int local_n = n / size;\n",
    "    int *local_array = (int*)malloc(local_n * sizeof(int));\n",
    "\n",
    "    MPI_Scatter(array, local_n, MPI_INT, local_array, local_n, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "    // Use OpenMP for parallel reduction to optimize the summing loop\n",
    "    #pragma omp parallel for reduction(+:local_sum)\n",
    "    for (int i = 0; i < local_n; i++) {\n",
    "        local_sum += local_array[i];\n",
    "    }\n",
    "\n",
    "    int total_sum = 0;\n",
    "    MPI_Reduce(&local_sum, &total_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "    if (rank == 0) {\n",
    "        printf(\"Total sum after optimization = %d\\\\n\", total_sum);\n",
    "        free(array);\n",
    "    }\n",
    "\n",
    "    free(local_array);\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Save the optimized C program to a file\n",
    "with open(\"mpi_sum_optimized.c\", \"w\") as file:\n",
    "    file.write(c_program_optimized)\n",
    "\n",
    "print(\"Optimized C program written to mpi_sum_optimized.c\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compile and Run the Optimized Program\n",
    "\n",
    "With the optimization in place, let's compile the program again, this time with OpenMP enabled, and then run it to see if there is an improvement in performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the optimized C program\n",
    "!mpicc -fopenmp -o mpi_sum_optimized mpi_sum_optimized.c\n",
    "\n",
    "# Run the optimized program with 4 processes\n",
    "!mpirun --allow-run-as-root -np 4 ./mpi_sum_optimized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Exercises\n",
    "\n",
    "In this notebook, we demonstrated a basic MPI program for parallel summation and then applied an optimization using OpenMP. \n",
    "\n",
    "### Exercises:\n",
    "1. **Increase the Array Size**: Modify the array size (`n`) and observe how the performance scales with larger arrays.\n",
    "2. **Compare Execution Time**: Measure the execution time of the original and optimized versions to quantify the performance improvement.\n",
    "3. **Parallelize Further**: Experiment with parallelizing other parts of the code or using different optimization techniques, such as loop unrolling or vectorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Solving a Simple Deadlock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A deadlock occurs when two or more processes wait indefinitely for each other to release resources, causing the program to hang. This is a common issue in parallel computing and can be difficult to detect and resolve.\n",
    "\n",
    "In this section, we'll simulate a simple deadlock using Python's `multiprocessing` library. We'll then debug the deadlock and modify the code to resolve it.\n",
    "\n",
    "### Part 1.1: Simulating a Deadlock\n",
    "\n",
    "Let's start by creating a scenario where two processes each wait for a lock held by the other, causing a deadlock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1 acquired lock1\n",
      "Process 2 acquired lock2\n",
      "Forcing termination due to deadlock...\n",
      "Forcing termination due to deadlock...\n",
      "Processes have been either completed or terminated.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def process_1(lock1, lock2):\n",
    "    with lock1:\n",
    "        print(\"Process 1 acquired lock1\")\n",
    "        time.sleep(1)\n",
    "        with lock2:\n",
    "            print(\"Process 1 acquired lock2\")\n",
    "    print(\"Process 1 completed\")\n",
    "\n",
    "def process_2(lock1, lock2):\n",
    "    with lock2:\n",
    "        print(\"Process 2 acquired lock2\")\n",
    "        time.sleep(1)\n",
    "        with lock1:\n",
    "            print(\"Process 2 acquired lock1\")\n",
    "    print(\"Process 2 completed\")\n",
    "\n",
    "def terminate_after_timeout(processes, timeout):\n",
    "    time.sleep(timeout)\n",
    "    for p in processes:\n",
    "        if p.is_alive():\n",
    "            print(\"Forcing termination due to deadlock...\")\n",
    "            p.terminate()\n",
    "            p.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create two locks\n",
    "    lock1 = mp.Lock()\n",
    "    lock2 = mp.Lock()\n",
    "\n",
    "    # Create and start two processes\n",
    "    p1 = mp.Process(target=process_1, args=(lock1, lock2))\n",
    "    p2 = mp.Process(target=process_2, args=(lock1, lock2))\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    # Start a monitoring thread to forcefully terminate the processes after 5 seconds\n",
    "    timeout_thread = threading.Thread(target=terminate_after_timeout, args=([p1, p2], 5))\n",
    "    timeout_thread.start()\n",
    "\n",
    "    # Wait for the processes to complete or be terminated\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "\n",
    "    print(\"Processes have been either completed or terminated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Identifying the Deadlock\n",
    "\n",
    "When you run the code, you'll notice that the program hangs. This is because both processes are waiting for each other to release the locks they hold, resulting in a deadlock.\n",
    "\n",
    "### Part 1.3: Resolving the Deadlock\n",
    "\n",
    "To resolve the deadlock, we need to ensure that both processes acquire the locks in the same order. This avoids the circular wait condition that leads to a deadlock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1 acquired lock1\n",
      "Process 1 acquired lock2\n",
      "Process 1 completedProcess 2 acquired lock1\n",
      "\n",
      "Process 2 acquired lock2\n",
      "Process 2 completed\n",
      "Both processes completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "def process_1_resolved(lock1, lock2):\n",
    "    with lock1:\n",
    "        print(\"Process 1 acquired lock1\")\n",
    "        time.sleep(1)\n",
    "        with lock2:\n",
    "            print(\"Process 1 acquired lock2\")\n",
    "    print(\"Process 1 completed\")\n",
    "\n",
    "def process_2_resolved(lock1, lock2):\n",
    "    with lock1:  # Changed from lock2 to lock1\n",
    "        print(\"Process 2 acquired lock1\")\n",
    "        time.sleep(1)\n",
    "        with lock2:\n",
    "            print(\"Process 2 acquired lock2\")\n",
    "    print(\"Process 2 completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create two locks\n",
    "    lock1 = mp.Lock()\n",
    "    lock2 = mp.Lock()\n",
    "\n",
    "    # Create and start two processes\n",
    "    p1 = mp.Process(target=process_1_resolved, args=(lock1, lock2))\n",
    "    p2 = mp.Process(target=process_2_resolved, args=(lock1, lock2))\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "\n",
    "    print(\"Both processes completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4: Explanation and Conclusion\n",
    "\n",
    "By ensuring that both processes acquire the locks in the same order, we avoid the circular wait condition that causes the deadlock. Now, the program completes successfully without hanging.\n",
    "\n",
    "Deadlocks are a common problem in parallel computing, but they can often be resolved by carefully managing the order in which locks are acquired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Solving a Simple Race Condition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A race condition occurs when the outcome of a program depends on the timing of uncontrollable events, such as the order in which threads or processes execute. This can lead to inconsistent or incorrect results.\n",
    "\n",
    "In this section, we'll simulate a simple race condition using Python's `multiprocessing` library. We'll then debug the issue and modify the code to resolve it.\n",
    "\n",
    "### Part 2.1: Simulating a Race Condition\n",
    "\n",
    "Let's start by creating a scenario where two processes update a shared variable simultaneously, leading to a race condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of shared_var: 19083\n"
     ]
    }
   ],
   "source": [
    "def increment(shared_var, lock):\n",
    "    for _ in range(10000):\n",
    "        shared_var.value += 1\n",
    "\n",
    "# Create a shared variable and a lock\n",
    "shared_var = mp.Value('i', 0)\n",
    "lock = mp.Lock()\n",
    "\n",
    "# Create and start two processes without using the lock (introducing a race condition)\n",
    "p1 = mp.Process(target=increment, args=(shared_var, lock))\n",
    "p2 = mp.Process(target=increment, args=(shared_var, lock))\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "print(f\"Final value of shared_var: {shared_var.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Identifying the Race Condition\n",
    "\n",
    "When you run the code, you'll notice that the final value of `shared_var` is often less than expected (i.e., not 20000). This is because the two processes are simultaneously updating the shared variable without proper synchronization, leading to a race condition.\n",
    "\n",
    "### Part 2.3: Resolving the Race Condition\n",
    "\n",
    "To resolve the race condition, we need to use a lock to ensure that only one process can update the shared variable at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of shared_var (with lock): 20000\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def increment_with_lock(shared_var, lock):\n",
    "    for _ in range(10000):\n",
    "        with lock:\n",
    "            shared_var.value += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Reset the shared variable to 0 before starting\n",
    "    shared_var = mp.Value('i', 0)\n",
    "    lock = mp.Lock()\n",
    "\n",
    "    # Create and start two processes with the lock to avoid the race condition\n",
    "    p1 = mp.Process(target=increment_with_lock, args=(shared_var, lock))\n",
    "    p2 = mp.Process(target=increment_with_lock, args=(shared_var, lock))\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "\n",
    "    print(f\"Final value of shared_var (with lock): {shared_var.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4: Explanation and Conclusion\n",
    "\n",
    "By using a lock, we ensure that only one process can access the shared variable at a time, thereby preventing the race condition. The final value of `shared_var` should now consistently be 20000.\n",
    "\n",
    "Race conditions can lead to unpredictable and incorrect behavior in parallel programs. Using synchronization mechanisms like locks is crucial to ensure that shared resources are accessed safely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end of the practice M3.P1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOFL+sc7nZMhbhn/M3UvvSW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
