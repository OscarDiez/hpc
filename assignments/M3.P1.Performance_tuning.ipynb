{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFL+sc7nZMhbhn/M3UvvSW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Performance Optimization in HPC\n","\n","## Introduction\n","\n","In this notebook, we will explore the fundamental techniques for optimizing code performance in High-Performance Computing (HPC) environments. Performance optimization is crucial for fully exploiting the capabilities of HPC architectures. By understanding and applying these techniques, you can significantly reduce the runtime of your computational tasks, making them more efficient and scalable.\n","\n","This practice is essential in HPC as it allows for better resource utilization, reduced costs, and the ability to solve larger and more complex problems. We will cover various optimization strategies, including code profiling, memory hierarchy optimization, and the use of high-performance libraries.\n","\n"],"metadata":{"id":"c8RhFcvahFkU"}},{"cell_type":"markdown","source":["## 2. Optimizing Code for HPC Architectures\n","\n","### 2.1 Code Profiling and Analysis\n","\n","Before optimizing any code, it's essential to understand where the bottlenecks are. Profiling tools help identify the most time-consuming parts of your code, which are the primary candidates for optimization.\n","\n","### 2.2 Loop Unrolling and Vectorization\n","\n","Loop unrolling and vectorization are common techniques used to enhance the performance of loops, which are often the most time-consuming parts of computational code.\n","\n","### 2.3 Memory Access Patterns and Cache Utilization\n","\n","Efficient memory access patterns and effective use of the CPU cache can dramatically speed up your programs.\n"],"metadata":{"id":"8fcZ7APohM10"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGyzMXXnftjM","executionInfo":{"status":"ok","timestamp":1724421332715,"user_tz":-120,"elapsed":958,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"073e3593-21f0-490c-971d-22a8dd69161c"},"outputs":[{"output_type":"stream","name":"stdout","text":["         5 function calls in 0.107 seconds\n","\n","   Ordered by: standard name\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","        1    0.105    0.105    0.105    0.105 <ipython-input-1-cda063ce1d20>:6(matrix_multiply)\n","        1    0.001    0.001    0.107    0.107 <string>:1(<module>)\n","        1    0.000    0.000    0.000    0.000 multiarray.py:741(dot)\n","        1    0.000    0.000    0.107    0.107 {built-in method builtins.exec}\n","        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n","\n","\n"]}],"source":["# Example: Profiling a simple matrix multiplication function using cProfile\n","\n","import cProfile\n","import numpy as np\n","\n","def matrix_multiply(A, B):\n","    return np.dot(A, B)\n","\n","# Create large random matrices\n","A = np.random.rand(1000, 1000)\n","B = np.random.rand(1000, 1000)\n","\n","# Profile the matrix multiplication\n","cProfile.run('matrix_multiply(A, B)')\n","\n","# The output will show where the time is being spent in the function\n"]},{"cell_type":"markdown","source":["### Explanation:\n","\n","The above code uses Python's `cProfile` to profile a matrix multiplication function. Profiling helps identify the parts of the code that consume the most computational resources, allowing us to focus our optimization efforts effectively.\n"],"metadata":{"id":"U_l2qcwJhX9N"}},{"cell_type":"markdown","source":["## 3. Memory Hierarchy and Data Locality\n","\n","### 3.1 Understanding Memory Hierarchy\n","\n","Memory hierarchy, from registers to cache and RAM, plays a critical role in the performance of HPC applications. Optimizing for memory hierarchy can significantly reduce data access times.\n","\n","### 3.2 Data Locality\n","\n","Data locality refers to the use of data elements within close proximity in memory, reducing cache misses and improving overall performance.\n"],"metadata":{"id":"CJXcLqLChcOc"}},{"cell_type":"code","source":["# Example: Measuring the impact of data locality on performance\n","\n","import time\n","\n","def sum_rows(matrix):\n","    total = 0\n","    for row in matrix:\n","        total += sum(row)\n","    return total\n","\n","def sum_columns(matrix):\n","    total = 0\n","    for col in range(matrix.shape[1]):\n","        total += sum(matrix[:, col])\n","    return total\n","\n","# Create a large matrix\n","matrix = np.random.rand(10000, 10000)\n","\n","# Measure row-wise sum performance\n","start_time = time.time()\n","sum_rows(matrix)\n","print(\"Row-wise sum time:\", time.time() - start_time)\n","\n","# Measure column-wise sum performance\n","start_time = time.time()\n","sum_columns(matrix)\n","print(\"Column-wise sum time:\", time.time() - start_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"afJj6SmghY-E","executionInfo":{"status":"ok","timestamp":1724421406179,"user_tz":-120,"elapsed":25777,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"219df990-271e-4aa9-f737-368e8322b8fe"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Row-wise sum time: 11.047656297683716\n","Column-wise sum time: 13.172894954681396\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","In the above example, we measure the performance impact of accessing matrix elements row-wise versus column-wise. Due to the way memory is structured, row-wise access is typically faster because it accesses contiguous memory locations, which is more cache-friendly.\n"],"metadata":{"id":"vcSAxdv4hf1c"}},{"cell_type":"markdown","source":["## 4. High-Performance Libraries for Scientific Computing\n","\n","Leveraging high-performance libraries can save development time and ensure that your code is optimized for modern HPC architectures.\n","\n","### 4.1 Using BLAS and LAPACK for Linear Algebra\n","\n","BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) are standard libraries that provide optimized implementations of basic linear algebra routines.\n"],"metadata":{"id":"Wsjf9ryRhqJF"}},{"cell_type":"code","source":["# Example: Using BLAS via NumPy for optimized matrix multiplication\n","\n","from scipy.linalg import blas\n","\n","# Using BLAS dgemm for matrix multiplication\n","C = blas.dgemm(1.0, A, B)\n","\n","print(\"Resulting matrix shape:\", C.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyvxXyG4hsA0","executionInfo":{"status":"ok","timestamp":1724421430380,"user_tz":-120,"elapsed":437,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"61ab16a4-78d3-4ba1-9667-b5ce15174e6f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Resulting matrix shape: (1000, 1000)\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","Here, we use the `dgemm` function from BLAS, accessed via SciPy, to perform matrix multiplication. This function is highly optimized for performance on many HPC systems, often outperforming custom implementations.\n"],"metadata":{"id":"zqcoRrtWhxzM"}},{"cell_type":"markdown","source":[],"metadata":{"id":"OuV7gdjJhzVl"}},{"cell_type":"markdown","source":["## 5. Parallel I/O and Data Management\n","\n","Efficient data management and parallel I/O are crucial for handling large datasets in HPC environments. This section introduces techniques to optimize I/O operations and manage data effectively.\n","\n","\n"],"metadata":{"id":"Bew1446Ah2PF"}},{"cell_type":"markdown","source":["\n","### 5.1 Installing Required Packages\n","\n","In this section, we'll install the necessary packages to perform parallel I/O operations in Google Colab using `mpi4py`.\n"],"metadata":{"id":"G25JmB84nf0i"}},{"cell_type":"code","source":["# Install the necessary MPI libraries and mpi4py package\n","!apt-get install -y libopenmpi-dev\n","!pip install mpi4py\n","\n","# Verifying the installation\n","from mpi4py import MPI\n","\n","print(\"mpi4py is successfully installed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WmYsrdWunjPT","executionInfo":{"status":"ok","timestamp":1724423259382,"user_tz":-120,"elapsed":254815,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"e891686a-574a-425c-99f4-e87b10ace0f0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n","Collecting mpi4py\n","  Downloading mpi4py-4.0.0.tar.gz (464 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.8/464.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mpi4py\n","  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpi4py: filename=mpi4py-4.0.0-cp310-cp310-linux_x86_64.whl size=4266257 sha256=aad712e3110e62b3d074ef92d20c123bd73792d1fbf84e578aca778908460467\n","  Stored in directory: /root/.cache/pip/wheels/96/17/12/83db63ee0ae5c4b040ee87f2e5c813aea4728b55ec6a37317c\n","Successfully built mpi4py\n","Installing collected packages: mpi4py\n","Successfully installed mpi4py-4.0.0\n","mpi4py is successfully installed.\n"]}]},{"cell_type":"markdown","source":["### 5.2 Parallel Filesystems\n","\n","Parallel filesystems like Lustre or GPFS are designed to provide high-throughput access to large datasets by allowing multiple processes to read/write data simultaneously.\n","\n","In this example, we'll use `mpi4py` to demonstrate simple parallel I/O. Note that running this code in a real HPC environment would involve a more complex setup, but this example provides a basic demonstration.\n"],"metadata":{"id":"SI1WMykbnlK6"}},{"cell_type":"code","source":["from mpi4py import MPI\n","import numpy as np\n","\n","# Initialize MPI\n","comm = MPI.COMM_WORLD\n","rank = comm.Get_rank()\n","size = comm.Get_size()\n","\n","# Create a large array on each process, each filled with the rank number\n","data = np.full(1000000, rank, dtype='i')\n","\n","# Write data to a shared file\n","fh = MPI.File.Open(comm, 'output.dat', MPI.MODE_CREATE | MPI.MODE_WRONLY)\n","fh.Write_at_all(rank * data.nbytes, data)\n","fh.Close()\n","\n","# Synchronize processes\n","comm.Barrier()\n","\n","# Reading data back collectively\n","collected_data = np.empty_like(data)\n","fh = MPI.File.Open(comm, 'output.dat', MPI.MODE_RDONLY)\n","fh.Read_at_all(rank * collected_data.nbytes, collected_data)\n","fh.Close()\n","\n","# Verify the data by printing a summary from each process\n","print(f\"Process {rank}: First element = {collected_data[0]}, Last element = {collected_data[-1]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZ6ZK6xDhy_8","executionInfo":{"status":"ok","timestamp":1724423598457,"user_tz":-120,"elapsed":427,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"10d2850a-79e0-4ef2-bb70-5f97906826ae"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Process 0: First element = 0, Last element = 0\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","In this example, we use `mpi4py` to perform parallel I/O, where each process writes its portion of data to a shared file. This is a simple demonstration of how parallel I/O can be implemented in an HPC environment.\n"],"metadata":{"id":"HyHg3HUXh7QF"}},{"cell_type":"markdown","source":["## 6. Introduction to Performance Tuning and Analysis\n","\n","Performance tuning and analysis are crucial for maximizing the efficiency of HPC applications. This section introduces the fundamental steps involved in performance tuning, including identifying bottlenecks, applying optimizations, and verifying improvements.\n","\n","### 6.1 Overview of Performance Tuning Steps\n","\n","The general workflow for performance tuning involves:\n","1. **Profiling the code** to identify performance bottlenecks.\n","2. **Applying optimizations** to the identified bottlenecks.\n","3. **Reprofiling the code** to assess the impact of the optimizations.\n","4. **Iterating** until performance goals are met.\n","\n","### 6.2 Setting Up the Environment\n","\n","We'll start by setting up the necessary environment for performance analysis, including installing profiling tools and libraries needed for the exercises.\n"],"metadata":{"id":"y9v6MP3Hob1D"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install line_profiler\n","!apt-get install -y libopenmpi-dev\n","!pip install mpi4py\n","\n","# Load the line_profiler extension\n","%load_ext line_profiler\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3wLg1NKpFEi","executionInfo":{"status":"ok","timestamp":1724423688169,"user_tz":-120,"elapsed":11213,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"ede28dc7-3c86-4700-ddfd-248a0d99906a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting line_profiler\n","  Downloading line_profiler-4.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n","Downloading line_profiler-4.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (717 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.6/717.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: line_profiler\n","Successfully installed line_profiler-4.1.3\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n","Requirement already satisfied: mpi4py in /usr/local/lib/python3.10/dist-packages (4.0.0)\n"]}]},{"cell_type":"markdown","source":["## 7. Profiling and Identifying Bottlenecks\n","\n","In this section, we'll profile a computational code to identify the most time-consuming parts. Profiling is the first step in any performance tuning process.\n","\n","### 7.1 Profiling with cProfile and line_profiler\n","\n","We'll use `cProfile` for an overall view of the code's performance and `line_profiler` for detailed line-by-line analysis.\n"],"metadata":{"id":"-8xwNdtYpH3q"}},{"cell_type":"code","source":["import cProfile\n","import numpy as np\n","\n","def compute_heavy_task(A, B):\n","    C = np.dot(A, B)\n","    D = np.linalg.inv(C)\n","    E = np.sum(D)\n","    return E\n","\n","# Create large random matrices\n","A = np.random.rand(1000, 1000)\n","B = np.random.rand(1000, 1000)\n","\n","# Profile the function\n","cProfile.run('compute_heavy_task(A, B)')\n","\n","# Detailed line profiling\n","%lprun -f compute_heavy_task compute_heavy_task(A, B)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPw0GkixpG0i","executionInfo":{"status":"ok","timestamp":1724423697165,"user_tz":-120,"elapsed":2073,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"19b4669a-940a-46e3-e99b-74b545e51218"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["         30 function calls in 0.713 seconds\n","\n","   Ordered by: standard name\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","        1    0.176    0.176    0.712    0.712 <ipython-input-2-fb907a52bf7a>:4(compute_heavy_task)\n","        1    0.001    0.001    0.713    0.713 <string>:1(<module>)\n","        1    0.000    0.000    0.000    0.000 fromnumeric.py:2172(_sum_dispatcher)\n","        1    0.000    0.000    0.010    0.010 fromnumeric.py:2177(sum)\n","        1    0.000    0.000    0.010    0.010 fromnumeric.py:71(_wrapreduction)\n","        1    0.000    0.000    0.000    0.000 fromnumeric.py:72(<dictcomp>)\n","        1    0.000    0.000    0.000    0.000 linalg.py:130(get_linalg_error_extobj)\n","        1    0.000    0.000    0.000    0.000 linalg.py:135(_makearray)\n","        2    0.000    0.000    0.000    0.000 linalg.py:140(isComplexType)\n","        1    0.000    0.000    0.000    0.000 linalg.py:153(_realType)\n","        1    0.000    0.000    0.000    0.000 linalg.py:159(_commonType)\n","        1    0.000    0.000    0.000    0.000 linalg.py:203(_assert_stacked_2d)\n","        1    0.000    0.000    0.000    0.000 linalg.py:209(_assert_stacked_square)\n","        1    0.000    0.000    0.000    0.000 linalg.py:488(_unary_dispatcher)\n","        1    0.526    0.526    0.526    0.526 linalg.py:492(inv)\n","        1    0.000    0.000    0.000    0.000 multiarray.py:741(dot)\n","        1    0.000    0.000    0.713    0.713 {built-in method builtins.exec}\n","        1    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n","        1    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n","        3    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n","        1    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n","        1    0.000    0.000    0.000    0.000 {method '__array_prepare__' of 'numpy.ndarray' objects}\n","        1    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n","        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n","        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n","        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n","        1    0.010    0.010    0.010    0.010 {method 'reduce' of 'numpy.ufunc' objects}\n","\n","\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","The code above uses `cProfile` to profile the entire function and `line_profiler` for a detailed line-by-line breakdown. This helps in identifying which parts of the code are the most time-consuming.\n","\n","### Exercise:\n","\n","Try modifying the `compute_heavy_task` function by adding other operations, such as matrix transposition or element-wise multiplication. Re-run the profiling tools to see how the performance characteristics change.\n"],"metadata":{"id":"CgKcVao9pLIC"}},{"cell_type":"markdown","source":["## 8. Applying Optimizations\n","\n","Once bottlenecks are identified, the next step is to apply optimizations. In this section, we will optimize matrix operations using techniques such as loop unrolling, vectorization, and memory access optimization.\n","\n","### 8.1 Loop Unrolling and Vectorization\n","\n","We will revisit loop unrolling and vectorization to see how they can improve performance in matrix operations.\n"],"metadata":{"id":"vds-N48VpMqS"}},{"cell_type":"code","source":["import numpy as np\n","import time\n","\n","def basic_matrix_sum(matrix):\n","    total = 0\n","    for i in range(matrix.shape[0]):\n","        for j in range(matrix.shape[1]):\n","            total += matrix[i, j]\n","    return total\n","\n","def vectorized_matrix_sum(matrix):\n","    return np.sum(matrix)\n","\n","# Create a large matrix\n","matrix = np.random.rand(10000, 10000)\n","\n","# Measure time for basic matrix sum\n","start_time = time.time()\n","basic_sum = basic_matrix_sum(matrix)\n","print(\"Basic matrix sum time:\", time.time() - start_time)\n","\n","# Measure time for vectorized matrix sum\n","start_time = time.time()\n","vectorized_sum = vectorized_matrix_sum(matrix)\n","print(\"Vectorized matrix sum time:\", time.time() - start_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhh0RdPMpN-7","executionInfo":{"status":"ok","timestamp":1724423753151,"user_tz":-120,"elapsed":31629,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"b9ed8497-4cf8-425c-84cd-571a70f08310"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Basic matrix sum time: 29.424485683441162\n","Vectorized matrix sum time: 0.08589839935302734\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","This example compares the performance of a basic loop-based matrix sum with a vectorized version using NumPy's built-in `sum` function. Vectorization allows for faster computation by leveraging SIMD instructions.\n","\n","### Exercise:\n","\n","Try optimizing the `basic_matrix_sum` function by manually unrolling the loops. Measure the performance impact and compare it with the vectorized approach.\n"],"metadata":{"id":"rvuGwJtopPWB"}},{"cell_type":"markdown","source":["## 9. Memory Access Optimization and Cache Utilization\n","\n","Memory access patterns greatly affect the performance of HPC applications. In this section, we'll explore techniques to optimize memory access and improve cache utilization.\n","\n","### 9.1 Optimizing Memory Access Patterns\n","\n","Efficient memory access patterns reduce cache misses, leading to faster execution times. We'll analyze the impact of row-major vs. column-major access.\n"],"metadata":{"id":"KAmDUSMPpRma"}},{"cell_type":"code","source":["import numpy as np\n","import time\n","\n","def row_major_sum(matrix):\n","    total = 0\n","    for i in range(matrix.shape[0]):\n","        for j in range(matrix.shape[1]):\n","            total += matrix[i, j]\n","    return total\n","\n","def column_major_sum(matrix):\n","    total = 0\n","    for j in range(matrix.shape[1]):\n","        for i in range(matrix.shape[0]):\n","            total += matrix[i, j]\n","    return total\n","\n","# Create a large matrix\n","matrix = np.random.rand(10000, 10000)\n","\n","# Measure row-major access time\n","start_time = time.time()\n","row_sum = row_major_sum(matrix)\n","print(\"Row-major sum time:\", time.time() - start_time)\n","\n","# Measure column-major access time\n","start_time = time.time()\n","column_sum = column_major_sum(matrix)\n","print(\"Column-major sum time:\", time.time() - start_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvLNYS3OpQkT","executionInfo":{"status":"ok","timestamp":1724423826332,"user_tz":-120,"elapsed":66767,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"4ce058a5-24b6-4f1b-ab9f-5e0c97b6ad3b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Row-major sum time: 28.65447998046875\n","Column-major sum time: 35.93936228752136\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","The example above compares row-major and column-major memory access patterns. Typically, row-major access is faster on most systems because it aligns better with how data is stored in memory.\n","\n","### Exercise:\n","\n","Modify the code to measure the cache hit rate (if possible using advanced profiling tools or libraries) for each access pattern. Observe how different matrix sizes affect cache utilization and performance.\n"],"metadata":{"id":"xbkdDKQjpUh6"}},{"cell_type":"markdown","source":["## 10. Leveraging High-Performance Libraries\n","\n","Using specialized HPC libraries can significantly enhance the performance of your applications. This section explores how to use BLAS, LAPACK, and other optimized libraries in your code.\n","\n","### 10.1 Using BLAS and LAPACK for Matrix Operations\n","\n","BLAS (Basic Linear Algebra Subprograms) and LAPACK are standard libraries providing highly optimized implementations of basic linear algebra routines.\n"],"metadata":{"id":"weiiRf7ppWCp"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.linalg import blas, lapack\n","\n","# Create large random matrices\n","A = np.random.rand(3, 3)  # Using smaller matrices for easier visualization\n","B = np.random.rand(3, 3)\n","\n","# Using BLAS dgemm for matrix multiplication\n","C = blas.dgemm(1.0, A, B)\n","\n","# Using LAPACK for matrix inversion (getrf followed by getri)\n","LU, piv, info = lapack.dgetrf(A)\n","inv_matrix, info = lapack.dgetri(LU, piv)\n","\n","# Display the results\n","print(\"Matrix A:\")\n","print(A)\n","\n","print(\"\\nMatrix B:\")\n","print(B)\n","\n","print(\"\\nResult of BLAS matrix multiplication (A * B = C):\")\n","print(C)\n","\n","print(\"\\nMatrix inversion of A using LAPACK:\")\n","print(inv_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tek106G0pXGC","executionInfo":{"status":"ok","timestamp":1724424101113,"user_tz":-120,"elapsed":466,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"c05ef701-d960-47a5-da21-27c1cb53ea80"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix A:\n","[[0.19885336 0.41443779 0.55232533]\n"," [0.63718034 0.84068626 0.05538211]\n"," [0.88803966 0.1892175  0.63410325]]\n","\n","Matrix B:\n","[[0.05203572 0.84679635 0.35407493]\n"," [0.77924956 0.11606839 0.82977504]\n"," [0.25053182 0.83445495 0.7396368 ]]\n","\n","Result of BLAS matrix multiplication (A * B = C):\n","[[0.47167302 0.67738203 0.82281926]\n"," [0.70213551 0.68335297 0.96415271]\n"," [0.35252048 1.30308151 0.94044664]]\n","\n","Matrix inversion of A using LAPACK:\n","[[-1.34380128  0.40701271  1.13494807]\n"," [ 0.91246533  0.93698744 -0.87662388]\n"," [ 1.60966597 -0.8496059   0.24916082]]\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","This example demonstrates how to use the BLAS `dgemm` function for matrix multiplication and the LAPACK `dgetrf` function for matrix inversion. These libraries are optimized for performance on many HPC systems.\n","\n","### Exercise:\n","\n","Try using other functions from BLAS and LAPACK, such as `dsymv` for symmetric matrix-vector multiplication or `dgeev` for eigenvalue computation. Compare the performance of these library functions with your custom implementations.\n"],"metadata":{"id":"30OdE7-6pZgR"}},{"cell_type":"markdown","source":["## 11. Advanced Performance Tuning with Parallel I/O\n","\n","Efficient I/O operations are critical for handling large datasets in HPC applications. This section covers advanced parallel I/O techniques using mpi4py.\n","\n","### 11.1 Implementing Parallel I/O\n","\n","We will extend our previous examples by implementing collective I/O operations, which can be more efficient for large-scale data processing.\n"],"metadata":{"id":"_vHHfgbkpc0B"}},{"cell_type":"code","source":["from mpi4py import MPI\n","import numpy as np\n","\n","comm = MPI.COMM_WORLD\n","rank = comm.Get_rank()\n","size = comm.Get_size()\n","\n","# Create a large array on each process\n","data = np.full(1000000, rank, dtype='i')\n","\n","# Write data collectively to a shared file\n","fh = MPI.File.Open(comm, 'collective_output.dat', MPI.MODE_CREATE | MPI.MODE_WRONLY)\n","fh.Write_at_all(rank * data.nbytes, data)\n","fh.Close()  # Manually close the file\n","\n","# Reading data collectively\n","collected_data = np.empty_like(data)\n","fh = MPI.File.Open(comm, 'collective_output.dat', MPI.MODE_RDONLY)\n","fh.Read_at_all(rank * collected_data.nbytes, collected_data)\n","fh.Close()  # Manually close the file after reading\n","\n","# Print out a summary of the data to verify the read operation\n","print(f\"Process {rank}: First element = {collected_data[0]}, Last element = {collected_data[-1]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZUOI0Fupbrs","executionInfo":{"status":"ok","timestamp":1724424207375,"user_tz":-120,"elapsed":452,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"e1526f35-a6b7-46d0-9021-4529f429fdd4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Process 0: First element = 0, Last element = 0\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","In this example, each MPI process writes and reads a portion of data from a shared file using collective I/O operations. This technique improves the efficiency of data handling in parallel applications.\n","\n","### Exercise:\n","\n","Modify the code to test the performance impact of different file access modes, such as `MPI.MODE_APPEND` or non-collective I/O. Analyze how these changes affect the scalability of I/O operations when running on multiple processes.\n"],"metadata":{"id":"Gcsx5P71pflx"}},{"cell_type":"markdown","source":["## 12. Comprehensive Performance Analysis and Tuning\n","\n","In this section, we will perform a comprehensive performance analysis and tuning of a complex HPC application. We will use profiling tools to identify bottlenecks and optimize the application.\n","\n","### 12.1 Case Study: Performance Tuning of a Scientific Application\n","\n","We will apply profiling, optimization, and parallel I/O techniques to a real-world scientific computation. The code will include matrix operations and parallel I/O.\n"],"metadata":{"id":"nXmUJRN9pima"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.linalg import blas, lapack\n","from mpi4py import MPI\n","import cProfile\n","import time\n","\n","# MPI setup\n","comm = MPI.COMM_WORLD\n","rank = comm.Get_rank()\n","size = comm.Get_size()\n","\n","# Problem size (reduced for better performance)\n","N = 500  # Smaller matrix size for quicker computation\n","\n","# Create large random matrices\n","A = np.random.rand(N, N)\n","B = np.random.rand(N, N)\n","\n","# Optimized computation\n","def optimized_computation(A, B):\n","    C = blas.dgemm(1.0, A, B)\n","    LU, piv, info = lapack.dgetrf(C)\n","    inv_matrix, info = lapack.dgetri(LU, piv)\n","    result = np.sum(inv_matrix)\n","    return result\n","\n","# Profile the optimized computation\n","cProfile.run('optimized_computation(A, B)')\n","\n","# Perform the computation\n","result = optimized_computation(A, B)\n","\n","# Parallel I/O to save results\n","file_handle = MPI.File.Open(comm, 'final_result.dat', MPI.MODE_CREATE | MPI.MODE_WRONLY)\n","result_array = np.array([result], dtype='d')\n","file_handle.Write_at_all(rank * result_array.nbytes, result_array)\n","file_handle.Close()\n","\n","# Final verification\n","print(f\"Process {rank} completed its task and saved the result. Result sum: {result}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18VZBQ0IpkKL","executionInfo":{"status":"ok","timestamp":1724424550186,"user_tz":-120,"elapsed":480,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"5f17a46a-56fa-4949-ee7b-979da91b4892"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["         11 function calls in 0.061 seconds\n","\n","   Ordered by: standard name\n","\n","   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n","        1    0.060    0.060    0.061    0.061 <ipython-input-11-f856f0057939>:20(optimized_computation)\n","        1    0.000    0.000    0.061    0.061 <string>:1(<module>)\n","        1    0.000    0.000    0.000    0.000 fromnumeric.py:2172(_sum_dispatcher)\n","        1    0.000    0.000    0.000    0.000 fromnumeric.py:2177(sum)\n","        1    0.000    0.000    0.000    0.000 fromnumeric.py:71(_wrapreduction)\n","        1    0.000    0.000    0.000    0.000 fromnumeric.py:72(<dictcomp>)\n","        1    0.000    0.000    0.061    0.061 {built-in method builtins.exec}\n","        1    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n","        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n","        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n","        1    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n","\n","\n","Process 0 completed its task and saved the result. Result sum: 7.163555117994207\n"]}]},{"cell_type":"markdown","source":["### Explanation:\n","\n","This case study brings together various optimization and parallelization techniques to solve a large-scale matrix problem. The code includes profiling, the use of high-performance libraries, and parallel I/O for saving the results.\n","\n","### Exercise:\n","\n","Expand the case study by adding more complex operations, such as eigenvalue computation or solving a system of linear equations. Profile and optimize these additional steps, and analyze how the performance scales with the problem size and number of processes.\n"],"metadata":{"id":"YiTwilzjpm6q"}},{"cell_type":"markdown","source":["## 13. MPI Programming in C with Performance Analysis\n","\n","In this section, we will write a simple MPI program in C, compile it, and run it directly within the Jupyter notebook. We will also perform basic profiling to analyze the performance.\n","\n","### 13.1 Writing the MPI Program in C\n","\n","First, we'll write a simple C program that initializes an array with the rank of each process, gathers all the data at the root process, and prints a summary.\n"],"metadata":{"id":"VwULBky0tMnj"}},{"cell_type":"code","source":["# Write the C program to a file\n","c_program = \"\"\"\n","#include <mpi.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","\n","int main(int argc, char** argv) {\n","    MPI_Init(&argc, &argv);\n","\n","    int rank, size;\n","    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n","    MPI_Comm_size(MPI_COMM_WORLD, &size);\n","\n","    // Create an array and initialize it with the rank of the process\n","    int array_size = 1000000;\n","    int* data = (int*)malloc(array_size * sizeof(int));\n","    for (int i = 0; i < array_size; i++) {\n","        data[i] = rank;\n","    }\n","\n","    // Gather data at the root process\n","    int* collected_data = NULL;\n","    if (rank == 0) {\n","        collected_data = (int*)malloc(size * array_size * sizeof(int));\n","    }\n","\n","    MPI_Gather(data, array_size, MPI_INT, collected_data, array_size, MPI_INT, 0, MPI_COMM_WORLD);\n","\n","    // Only the root process will print the first and last elements of each process's data\n","    if (rank == 0) {\n","        for (int i = 0; i < size; i++) {\n","            printf(\"Process %d: First element = %d, Last element = %d\\\\n\", i, collected_data[i * array_size], collected_data[(i+1) * array_size - 1]);\n","        }\n","        free(collected_data);\n","    }\n","\n","    free(data);\n","    MPI_Finalize();\n","\n","    return 0;\n","}\n","\"\"\"\n","\n","# Save the C program to a file\n","with open(\"mpi_example.c\", \"w\") as file:\n","    file.write(c_program)\n","\n","print(\"C program written to mpi_example.c\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uy4MITVEtPk_","executionInfo":{"status":"ok","timestamp":1724424563448,"user_tz":-120,"elapsed":451,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"f14f46b6-ea48-46d9-a68d-94f052849ea7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["C program written to mpi_example.c\n"]}]},{"cell_type":"markdown","source":["### 13.2 Compiling the C Program\n","\n","Next, we will compile the C program using `mpicc`. This is done directly in the notebook using shell commands.\n"],"metadata":{"id":"FsNcVSc2tQTb"}},{"cell_type":"code","source":["# Compile the C program using mpicc\n","!mpicc -o mpi_example mpi_example.c\n","\n","# Check if the compilation was successful\n","!ls -l mpi_example\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxTcpQ_StUlz","executionInfo":{"status":"ok","timestamp":1724424569132,"user_tz":-120,"elapsed":455,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"b7e80096-a84e-46cd-ef65-0a2a538caa63"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["-rwxr-xr-x 1 root root 16368 Aug 23 14:49 mpi_example\n"]}]},{"cell_type":"markdown","source":["### 13.3 Running the Program\n","\n","Now, we will run the compiled program using `mpirun` or `mpiexec`. We will specify the number of processes using the `-np` flag.\n"],"metadata":{"id":"IOzf0qxktU-o"}},{"cell_type":"code","source":["# Use the --allow-run-as-root flag with mpirun\n","!mpirun --allow-run-as-root -np 4 ./mpi_example\n","\n"],"metadata":{"id":"NOEOReY0tW8i","executionInfo":{"status":"ok","timestamp":1724424647946,"user_tz":-120,"elapsed":469,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### 13.4 Profiling the Program\n","\n","To profile the program, we can use `gprof`. This section will involve compiling the program with profiling enabled, running it to generate profile data, and then analyzing that data.\n","\n","### Profiling with gprof\n","\n","Compile the program with the `-pg` flag, which enables profiling:\n"],"metadata":{"id":"aeG9hlPutZWj"}},{"cell_type":"code","source":["# Compile with profiling enabled\n","!mpicc -pg -o mpi_example_profiled mpi_example.c\n","\n","# Run the program (this generates the profiling data file gmon.out)\n","!mpirun -np 4 ./mpi_example_profiled\n","\n","# Analyze the profiling data\n","!gprof ./mpi_example_profiled gmon.out > analysis.txt\n","\n","# Display the profiling results\n","!cat analysis.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4V-tsoAwtavr","executionInfo":{"status":"ok","timestamp":1724424723415,"user_tz":-120,"elapsed":1078,"user":{"displayName":"Oscar Diez","userId":"09940749782853693007"}},"outputId":"b1d8b1e7-7dd2-4ce5-80c0-def4a2b87374"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------\n","mpirun has detected an attempt to run as root.\n","\n","Running as root is *strongly* discouraged as any mistake (e.g., in\n","defining TMPDIR) or bug can result in catastrophic damage to the OS\n","file system, leaving your system in an unusable state.\n","\n","We strongly suggest that you run mpirun as a non-root user.\n","\n","You can override this protection by adding the --allow-run-as-root option\n","to the cmd line or by setting two environment variables in the following way:\n","the variable OMPI_ALLOW_RUN_AS_ROOT=1 to indicate the desire to override this\n","protection, and OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1 to confirm the choice and\n","add one more layer of certainty that you want to do so.\n","We reiterate our advice against doing so - please proceed at your own risk.\n","--------------------------------------------------------------------------\n","gmon.out: No such file or directory\n"]}]},{"cell_type":"markdown","source":["In this section, we wrote a simple MPI program in C, compiled it, and ran it directly within the Jupyter notebook. We also performed basic profiling using `gprof` to analyze the program's performance. This exercise demonstrated the end-to-end process of developing, running, and profiling an MPI-based HPC application.\n","\n","### Exercises:\n","\n","- **Modify the C Program**: Try increasing the size of the array or changing the type of data being processed, and observe how these changes impact performance as reported by `gprof`.\n","- **Explore Advanced Profiling Tools**: For more detailed analysis, consider using tools like `perf` or `Intel VTune` to gain deeper insights into your program's performance.\n"],"metadata":{"id":"t45EBc-GtcYL"}},{"cell_type":"markdown","source":[],"metadata":{"id":"CNSW3EdgzpbR"}}]}